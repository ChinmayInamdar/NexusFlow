{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b89d23a-79bc-4903-a8c3-d1d351d7a3f5",
   "metadata": {},
   "source": [
    "#  Data Engineering Challenge - Take Home Assignment\n",
    "\n",
    "Having joined TechCorp as a Data Engineering Intern, the task is to create a unified data pipeline that can ETL and analyse data from three recently acquired companies. \n",
    "Data being in seperate formats with many inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80168e3-b5c9-49e7-bc98-b7168e6bbd3c",
   "metadata": {},
   "source": [
    "# General approach\n",
    "\n",
    "Phase 1: \n",
    "- Explore each dataset and find data quality issues\n",
    "- Identify and understand relationships between tables\n",
    "- Make the ER model\n",
    "- Have a clear data cleaning strategy\n",
    "\n",
    "Phase 2: \n",
    "- Create data cleaning functions\n",
    "- Handle edge cases\n",
    "- implement data validation\n",
    "- Design normalised schema\n",
    "- Load into SQLite DB\n",
    "\n",
    "Phase 3: \n",
    "- Build UI using Streamlit\n",
    "- Create visualizations\n",
    "- Have filtering and search\n",
    "\n",
    "Phase 4: \n",
    "- Use Gemini AI to reconcile records\n",
    "- Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c938833-dacc-45a1-a36e-ae5e4cfcc43b",
   "metadata": {},
   "source": [
    "# Initial setup: imports libraries, configures file paths, logging, Pandas display settings, and logs completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebc1558-861f-4899-9cba-a46aa10506dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:41,939 - INFO - Setup and configuration complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from dateutil import parser # For more robust date parsing\n",
    "\n",
    "# --- Configuration ---\n",
    "# File paths\n",
    "DATA_DIR = './' # Assuming data files are in the same directory as the notebook\n",
    "RECONCILIATION_DATA_CSV = os.path.join(DATA_DIR, 'data/reconciliation_challenge_data.csv')\n",
    "PRODUCTS_INCONSISTENT_JSON = os.path.join(DATA_DIR, 'data/products_inconsistent_data.json')\n",
    "CUSTOMERS_MESSY_JSON = os.path.join(DATA_DIR, 'data/customers_messy_data.json')\n",
    "ORDERS_UNSTRUCTURED_CSV = os.path.join(DATA_DIR, 'data/orders_unstructured_data.csv')\n",
    "\n",
    "DB_NAME = 'unified_ecommerce.db'\n",
    "DB_PATH = os.path.join(DATA_DIR, DB_NAME) # Save DB in the data directory\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler()]) # Outputs to console; can add FileHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "logger.info(\"Setup and configuration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d498f16-ffe5-4f53-b4f6-837c57382e0c",
   "metadata": {},
   "source": [
    "# 1. Phase 1: Data Discovery & Analysis (Jupyter Notebook)\n",
    "1.1 Load Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3e8bb-1bca-4901-b614-36ebad9f9997",
   "metadata": {},
   "source": [
    "# Customer Data (customers_messy_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c10212-5f62-4a2f-9404-01aefcedda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:41,945 - INFO - Loading Customer data...\n",
      "2025-06-23 08:48:41,956 - INFO - Customer data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading Customer data...\")\n",
    "try:\n",
    "    with open(CUSTOMERS_MESSY_JSON, 'r') as f:\n",
    "        raw_customers_data = json.load(f)\n",
    "    df_customers_raw = pd.DataFrame(raw_customers_data)\n",
    "    logger.info(\"Customer data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Customer data file not found at {CUSTOMERS_MESSY_JSON}\")\n",
    "    df_customers_raw = pd.DataFrame() # Create empty DataFrame to avoid downstream errors\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"Error decoding JSON from {CUSTOMERS_MESSY_JSON}\")\n",
    "    df_customers_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840b18d-7516-480c-b97d-aec59c0733a9",
   "metadata": {},
   "source": [
    "# Product Data (products_inconsistent_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13a6482-74d3-41f5-86ea-4ddc8d680d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:41,965 - INFO - Loading Product data...\n",
      "2025-06-23 08:48:41,975 - INFO - Product data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading Product data...\")\n",
    "try:\n",
    "    with open(PRODUCTS_INCONSISTENT_JSON, 'r') as f:\n",
    "        raw_products_data = json.load(f)\n",
    "    df_products_raw = pd.DataFrame(raw_products_data)\n",
    "    logger.info(\"Product data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Product data file not found at {PRODUCTS_INCONSISTENT_JSON}\")\n",
    "    df_products_raw = pd.DataFrame()\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"Error decoding JSON from {PRODUCTS_INCONSISTENT_JSON}\")\n",
    "    df_products_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db71bcd-2d73-4458-8cfc-bbecec8d2526",
   "metadata": {},
   "source": [
    "# Reconciliation Data (reconciliation_challenge_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daea855a-ac84-4c93-8d6b-5119f506a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:41,986 - INFO - Loading Reconciliation (Order Items Source 1) data...\n",
      "2025-06-23 08:48:41,996 - INFO - Reconciliation data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading Reconciliation (Order Items Source 1) data...\")\n",
    "try:\n",
    "    df_reconciliation_raw = pd.read_csv(RECONCILIATION_DATA_CSV)\n",
    "    logger.info(\"Reconciliation data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Reconciliation data file not found at {RECONCILIATION_DATA_CSV}\")\n",
    "    df_reconciliation_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bb26f-1b19-4be5-9102-055850e53e84",
   "metadata": {},
   "source": [
    "# Orders Unstructured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2f336d-ea93-4388-947f-7f59500eee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,007 - INFO - Loading Orders Unstructured (Order Items Source 2) data...\n",
      "2025-06-23 08:48:42,021 - INFO - Orders Unstructured data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading Orders Unstructured (Order Items Source 2) data...\")\n",
    "try:\n",
    "    df_orders_unstructured_raw = pd.read_csv(ORDERS_UNSTRUCTURED_CSV)\n",
    "    logger.info(\"Orders Unstructured data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Orders Unstructured data file not found at {ORDERS_UNSTRUCTURED_CSV}\")\n",
    "    df_orders_unstructured_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50957f-8c01-4782-899d-a34393637f61",
   "metadata": {},
   "source": [
    "# 1.2 Initial Exploration \n",
    "\n",
    "The initial exploration summarizes the structure, missing values, duplicates, and data types of the raw customer dataset. It also analyzes key columns with value counts and descriptive statistics to assess data quality and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1c759-4ba0-46dd-9256-4de055ddd4a9",
   "metadata": {},
   "source": [
    "# Exploration for df_customers_raw (Customer Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86168c9e-371f-4d04-b1b7-966040a998ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,036 - INFO - \n",
      "--- Exploring Raw Customer Data (customers_messy_data.json) ---\n",
      "2025-06-23 08:48:42,126 - INFO - Value counts for key categorical/ID columns in Customers:\n",
      "2025-06-23 08:48:42,139 - INFO - Customer data exploration complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (500, 25)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 25 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   customer_id        500 non-null    object \n",
      " 1   cust_id            500 non-null    object \n",
      " 2   customer_name      500 non-null    object \n",
      " 3   full_name          500 non-null    object \n",
      " 4   email              409 non-null    object \n",
      " 5   email_address      500 non-null    object \n",
      " 6   phone              187 non-null    object \n",
      " 7   phone_number       500 non-null    object \n",
      " 8   address            500 non-null    object \n",
      " 9   city               500 non-null    object \n",
      " 10  state              500 non-null    object \n",
      " 11  zip_code           361 non-null    object \n",
      " 12  postal_code        500 non-null    object \n",
      " 13  registration_date  412 non-null    object \n",
      " 14  reg_date           500 non-null    object \n",
      " 15  status             443 non-null    object \n",
      " 16  customer_status    436 non-null    object \n",
      " 17  total_orders       500 non-null    int64  \n",
      " 18  total_spent        500 non-null    object \n",
      " 19  loyalty_points     500 non-null    int64  \n",
      " 20  preferred_payment  406 non-null    object \n",
      " 21  age                313 non-null    float64\n",
      " 22  birth_date         197 non-null    object \n",
      " 23  gender             421 non-null    object \n",
      " 24  segment            405 non-null    object \n",
      "dtypes: float64(1), int64(2), object(22)\n",
      "memory usage: 97.8+ KB\n",
      "\n",
      "First 5 rows:\n",
      "  customer_id    cust_id   customer_name      full_name                  email     email_address     phone    phone_number          address      city     state zip_code postal_code registration_date    reg_date     status customer_status  total_orders total_spent  loyalty_points preferred_payment   age  birth_date  gender  segment\n",
      "0           1  CUST_0001       Grace Lee       jane_doe  customer1@example.com  user1@domain.com      None  (555) 584-4439    2010 First St        la  New York     None  46867-5820        2023-08-26               INACTIVE        INACTIVE             1     4961.60             867              cash   NaN  1977-03-09          regular\n",
      "1           2  CUST_0002    diana.prince   frank-miller                   None                        None                    9094 First St   Chicago        IL    44794  80300-9308         7/24/2023  2021-08-01  suspended       suspended            18     3484.74              33              cash   NaN        None  Female      vip\n",
      "2           3  CUST_0003       EVE WHITE      EVE WHITE  customer3@example.com                        None  (555) 614-7353     8591 Pine Rd   Phoenix        AZ    64901                          None                 active        INACTIVE            15      328.23             266       credit_card  72.0  1997-02-07       M  premium\n",
      "3           4  CUST_0004   Charlie Brown  Charlie Brown  customer4@example.com  user4@domain.com      None                  1508 Second Ave  new_york        NY    99995                     8/28/2023  2021-05-15  suspended         pending            11      834.41             584              cash  72.0        None             None\n",
      "4           5  CUST_0005  henry.davis123   diana.prince  customer5@example.com  user5@domain.com  555-1431                     4610 Main St        la        IL    48460  33933-7518        2023-07-31  2021-10-18   INACTIVE        inactive             9     1412.94              17       credit_card  18.0  1997-07-02   Other  premium\n",
      "\n",
      "Last 5 rows:\n",
      "    customer_id    cust_id   customer_name      full_name                    email       email_address     phone    phone_number        address          city state zip_code postal_code registration_date    reg_date     status customer_status  total_orders total_spent  loyalty_points preferred_payment   age  birth_date  gender  segment\n",
      "495         496  CUST_0496      John Smith  Charlie Brown  customer496@example.com                          None  (555) 784-1304  4430 First St       Chicago    TX    18735  46606-1899         3/16/2021  2022-07-07   inactive          ACTIVE            27     4424.10             783              cash  56.0  1963-06-11       F  regular\n",
      "496         497  CUST_0497       EVE WHITE   frank-miller                     None  user497@domain.com      None                   8166 Oak Ave       chicago    IL    91679  40220-9691        2023-05-02  2023-06-20     active                            27     3572.96             516            paypal  26.0  1994-10-27    None      new\n",
      "497         498  CUST_0498    diana.prince       jane_doe                     None  user498@domain.com  555-7315                   1857 Pine Rd  philadelphia    CA    56580                          None  2020-12-16     ACTIVE       suspended             8     3365.96             565              cash   NaN        None       M      new\n",
      "498         499  CUST_0499  henry.davis123     John Smith  customer499@example.com                          None                   2694 Main St            la    TX    33967  16024-9773        12/25/2023               inactive          active            17      861.57             874              None  36.0  1997-09-12  Female  regular\n",
      "499         500  CUST_0500       Grace Lee   diana.prince  customer500@example.com  user500@domain.com      None                   8524 Pine Rd           NYC    NY    14475  84948-1789         8/12/2021  2022-10-10  suspended         pending            24     4199.18             228              None   NaN        None    Male  premium\n",
      "\n",
      "Data Types:\n",
      "customer_id           object\n",
      "cust_id               object\n",
      "customer_name         object\n",
      "full_name             object\n",
      "email                 object\n",
      "email_address         object\n",
      "phone                 object\n",
      "phone_number          object\n",
      "address               object\n",
      "city                  object\n",
      "state                 object\n",
      "zip_code              object\n",
      "postal_code           object\n",
      "registration_date     object\n",
      "reg_date              object\n",
      "status                object\n",
      "customer_status       object\n",
      "total_orders           int64\n",
      "total_spent           object\n",
      "loyalty_points         int64\n",
      "preferred_payment     object\n",
      "age                  float64\n",
      "birth_date            object\n",
      "gender                object\n",
      "segment               object\n",
      "dtype: object\n",
      "\n",
      "Missing values (count and percentage):\n",
      "                   count  percentage\n",
      "email                 91        18.2\n",
      "phone                313        62.6\n",
      "zip_code             139        27.8\n",
      "registration_date     88        17.6\n",
      "status                57        11.4\n",
      "customer_status       64        12.8\n",
      "preferred_payment     94        18.8\n",
      "age                  187        37.4\n",
      "birth_date           303        60.6\n",
      "gender                79        15.8\n",
      "segment               95        19.0\n",
      "\n",
      "Duplicate rows (entire row): 0\n",
      "\n",
      "Descriptive statistics (numerical columns):\n",
      "       total_orders  loyalty_points         age\n",
      "count    500.000000      500.000000  313.000000\n",
      "mean      23.200000      502.724000   47.236422\n",
      "std       14.329224      288.656885   17.791213\n",
      "min        0.000000        3.000000   18.000000\n",
      "25%       11.000000      258.250000   32.000000\n",
      "50%       22.500000      508.000000   46.000000\n",
      "75%       35.250000      746.500000   62.000000\n",
      "max       49.000000      999.000000   77.000000\n",
      "\n",
      "Descriptive statistics (object/categorical columns):\n",
      "       customer_id    cust_id customer_name  full_name                  email email_address     phone phone_number       address      city state zip_code postal_code registration_date reg_date   status customer_status total_spent preferred_payment  birth_date gender segment\n",
      "count          500        500           500        500                    409           500       187          500           500       500   500      361         500               412      500      443             436         500               406         197    421     405\n",
      "unique         500        500            10         10                    409           339       184          245           497        10     8      361         311               361      308        7               7         499                 4         197      6       4\n",
      "top              1  CUST_0001    John Smith  EVE WHITE  customer1@example.com                555-4748               2800 Pine Rd  New York    NY    44794                      8/9/2021           pending          active     4056.98       credit_card  1977-03-09            vip\n",
      "freq             1          1            67         66                      1           162         2          256             2        61    73        1         190                 3      155       68              71           2               108           1     92     110\n",
      "\n",
      "Value counts for 'cust_id':\n",
      "cust_id\n",
      "CUST_0001    1\n",
      "CUST_0002    1\n",
      "CUST_0003    1\n",
      "CUST_0004    1\n",
      "CUST_0005    1\n",
      "CUST_0006    1\n",
      "CUST_0007    1\n",
      "CUST_0008    1\n",
      "CUST_0009    1\n",
      "CUST_0010    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'cust_id': 500\n",
      "\n",
      "Value counts for 'customer_name':\n",
      "customer_name\n",
      "John Smith                 67\n",
      "Charlie Brown              57\n",
      "frank-miller               54\n",
      "Bob Wilson                 50\n",
      "henry.davis123             49\n",
      "Grace Lee                  48\n",
      "jane_doe                   48\n",
      "EVE WHITE                  45\n",
      "alice.johnson@email.com    45\n",
      "diana.prince               37\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'customer_name': 10\n",
      "\n",
      "Value counts for 'full_name':\n",
      "full_name\n",
      "EVE WHITE                  66\n",
      "John Smith                 55\n",
      "frank-miller               54\n",
      "henry.davis123             51\n",
      "diana.prince               50\n",
      "Grace Lee                  49\n",
      "Bob Wilson                 47\n",
      "jane_doe                   45\n",
      "alice.johnson@email.com    43\n",
      "Charlie Brown              40\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'full_name': 10\n",
      "\n",
      "Value counts for 'email':\n",
      "email\n",
      "None                       91\n",
      "customer342@example.com     1\n",
      "customer341@example.com     1\n",
      "customer340@example.com     1\n",
      "customer339@example.com     1\n",
      "customer338@example.com     1\n",
      "customer337@example.com     1\n",
      "customer336@example.com     1\n",
      "customer335@example.com     1\n",
      "customer334@example.com     1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'email': 410\n",
      "\n",
      "Value counts for 'email_address':\n",
      "email_address\n",
      "                      162\n",
      "user333@domain.com      1\n",
      "user332@domain.com      1\n",
      "user327@domain.com      1\n",
      "user326@domain.com      1\n",
      "user325@domain.com      1\n",
      "user324@domain.com      1\n",
      "user323@domain.com      1\n",
      "user322@domain.com      1\n",
      "user319@domain.com      1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'email_address': 339\n",
      "\n",
      "Value counts for 'city':\n",
      "city\n",
      "New York        61\n",
      "Los Angeles     59\n",
      "la              57\n",
      "Chicago         53\n",
      "NYC             49\n",
      "chicago         47\n",
      "Phoenix         45\n",
      "new_york        43\n",
      "philadelphia    43\n",
      "Houston         43\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'city': 10\n",
      "\n",
      "Value counts for 'state':\n",
      "state\n",
      "NY            73\n",
      "IL            70\n",
      "New York      67\n",
      "CA            63\n",
      "AZ            62\n",
      "PA            59\n",
      "California    55\n",
      "TX            51\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'state': 8\n",
      "\n",
      "Value counts for 'status':\n",
      "status\n",
      "pending      68\n",
      "suspended    67\n",
      "ACTIVE       64\n",
      "active       64\n",
      "inactive     61\n",
      "             60\n",
      "INACTIVE     59\n",
      "None         57\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'status': 8\n",
      "\n",
      "Value counts for 'customer_status':\n",
      "customer_status\n",
      "active       71\n",
      "inactive     68\n",
      "None         64\n",
      "suspended    64\n",
      "             61\n",
      "ACTIVE       59\n",
      "pending      57\n",
      "INACTIVE     56\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'customer_status': 8\n",
      "\n",
      "Value counts for 'gender':\n",
      "gender\n",
      "          92\n",
      "None      79\n",
      "M         71\n",
      "Female    67\n",
      "F         65\n",
      "Other     63\n",
      "Male      63\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'gender': 7\n",
      "\n",
      "Value counts for 'segment':\n",
      "segment\n",
      "vip        110\n",
      "new        107\n",
      "premium    100\n",
      "None        95\n",
      "regular     88\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'segment': 5\n",
      "\n",
      "Value counts for 'preferred_payment':\n",
      "preferred_payment\n",
      "credit_card    108\n",
      "paypal         106\n",
      "cash            98\n",
      "debit_card      94\n",
      "None            94\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'preferred_payment': 5\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Exploring Raw Customer Data (customers_messy_data.json) ---\")\n",
    "if not df_customers_raw.empty:\n",
    "    print(\"Shape:\", df_customers_raw.shape)\n",
    "    \n",
    "    print(\"\\nInfo:\")\n",
    "    df_customers_raw.info()\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_customers_raw.head())\n",
    "    \n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_customers_raw.tail())\n",
    "    \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df_customers_raw.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values (count and percentage):\")\n",
    "    missing_values = df_customers_raw.isnull().sum()\n",
    "    missing_percentage = (df_customers_raw.isnull().sum() / len(df_customers_raw)) * 100\n",
    "    missing_df = pd.DataFrame({'count': missing_values, 'percentage': missing_percentage})\n",
    "    print(missing_df[missing_df['count'] > 0])\n",
    "    \n",
    "    print(\"\\nDuplicate rows (entire row):\", df_customers_raw.duplicated().sum())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (numerical columns):\")\n",
    "    print(df_customers_raw.describe(include=[np.number]))\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (object/categorical columns):\")\n",
    "    print(df_customers_raw.describe(include=['object']))\n",
    "    \n",
    "    logger.info(\"Value counts for key categorical/ID columns in Customers:\")\n",
    "    key_customer_cols = ['cust_id', 'customer_name', 'full_name', 'email', 'email_address', \n",
    "                         'city', 'state', 'status', 'customer_status', 'gender', 'segment', 'preferred_payment']\n",
    "    for col in key_customer_cols:\n",
    "        if col in df_customers_raw.columns:\n",
    "            print(f\"\\nValue counts for '{col}':\")\n",
    "            print(df_customers_raw[col].value_counts(dropna=False).head(10)) # Show top 10 + NaN\n",
    "            print(f\"Number of unique values in '{col}': {df_customers_raw[col].nunique(dropna=False)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Column '{col}' not found in raw customer data.\")\n",
    "    \n",
    "    logger.info(\"Customer data exploration complete.\")\n",
    "else:\n",
    "    logger.warning(\"Customer DataFrame (df_customers_raw) is empty. Skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149fc6e-4db5-481b-8a2e-b4b6f70f89fc",
   "metadata": {},
   "source": [
    "# Exploration for df_products_raw (Product Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf74ce85-d9ec-4244-8130-87f1f310b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,151 - INFO - \n",
      "--- Exploring Raw Product Data (products_inconsistent_data.json) ---\n",
      "2025-06-23 08:48:42,231 - INFO - Value counts for key categorical/ID columns in Products:\n",
      "2025-06-23 08:48:42,241 - INFO - Product data exploration complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (200, 24)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   product_id        200 non-null    object\n",
      " 1   item_id           200 non-null    int64 \n",
      " 2   product_name      200 non-null    object\n",
      " 3   item_name         200 non-null    object\n",
      " 4   description       145 non-null    object\n",
      " 5   category          200 non-null    object\n",
      " 6   product_category  200 non-null    object\n",
      " 7   brand             171 non-null    object\n",
      " 8   manufacturer      173 non-null    object\n",
      " 9   price             200 non-null    object\n",
      " 10  list_price        200 non-null    object\n",
      " 11  cost              200 non-null    object\n",
      " 12  weight            123 non-null    object\n",
      " 13  dimensions        200 non-null    object\n",
      " 14  color             170 non-null    object\n",
      " 15  size              170 non-null    object\n",
      " 16  stock_quantity    200 non-null    int64 \n",
      " 17  stock_level       200 non-null    int64 \n",
      " 18  reorder_level     200 non-null    int64 \n",
      " 19  supplier_id       153 non-null    object\n",
      " 20  created_date      171 non-null    object\n",
      " 21  last_updated      200 non-null    object\n",
      " 22  is_active         200 non-null    object\n",
      " 23  rating            138 non-null    object\n",
      "dtypes: int64(4), object(20)\n",
      "memory usage: 37.6+ KB\n",
      "\n",
      "First 5 rows:\n",
      "  product_id  item_id product_name    item_name                description     category product_category    brand manufacturer   price list_price    cost weight dimensions  color      size  stock_quantity  stock_level  reorder_level supplier_id created_date              last_updated is_active rating\n",
      "0   PROD_001        1    Product 1  Item 1 Name  Description for product 1       Sports         clothing  BRAND-C       brandE  241.07     543.15   61.01   3.22   44x49x38   None      None             994          228             24      SUP_06   2020-10-11  2023-10-06T18:30:00.000Z       yes    1.4\n",
      "1   PROD_002        2    Product 2  Item 2 Name  Description for product 2     clothing             Toys  BRAND-C      BRAND-C  429.55      34.68  126.98   6.29   14x19x37  Black         L             631          365             46      SUP_14   2022-06-26  2023-09-05T18:30:00.000Z        no    2.2\n",
      "2   PROD_003        3    Product 3  Item 3 Name                       None     clothing             Toys               brand_b  423.62     287.41  195.55   5.37             White                       640          345             19        None   2020-11-27                                  no    2.5\n",
      "3   PROD_004        4    Product 4  Item 4 Name  Description for product 4  Electronics    Home & Garden  brand_b       BrandA   86.17     321.23  281.95   6.48             White         L             194           61             35        None   2022-03-06  2023-05-07T18:30:00.000Z      True    4.5\n",
      "4   PROD_005        5    Product 5  Item 5 Name  Description for product 5     clothing    Home & Garden               Brand D  454.24     161.26  282.38   7.34    48x28x5         One Size             503          116             34      SUP_20   2023-02-08  2023-07-25T18:30:00.000Z      true    0.8\n",
      "\n",
      "Last 5 rows:\n",
      "    product_id  item_id product_name      item_name                  description     category product_category    brand manufacturer   price list_price    cost weight dimensions  color  size  stock_quantity  stock_level  reorder_level supplier_id created_date              last_updated is_active rating\n",
      "195   PROD_196      196  Product 196  Item 196 Name  Description for product 196       Sports      electronics  brand_b       brandE  493.13     101.28  243.43   2.94              None     S             705          200             32      SUP_03   2021-11-03  2023-05-21T18:30:00.000Z     false   None\n",
      "196   PROD_197      197  Product 197  Item 197 Name  Description for product 197     clothing      Electronics   brandE      brand_b  447.55     107.33  270.31   0.26             Black    XL              19          370             26      SUP_17   2020-06-08  2023-03-16T18:30:00.000Z     False    1.3\n",
      "197   PROD_198      198  Product 198  Item 198 Name  Description for product 198  Electronics      electronics  brand_b       BrandA   28.08     559.45  124.41   9.11   30x27x40  Black     M              26          233              6      SUP_11   2023-02-20                                true    3.5\n",
      "198   PROD_199      199  Product 199  Item 199 Name  Description for product 199       Sports         clothing     None               111.23     527.16   61.54   None   21x46x34  White  None             918          245             15      SUP_09   2020-08-31  2023-02-02T18:30:00.000Z         1    1.8\n",
      "199   PROD_200      200  Product 200  Item 200 Name  Description for product 200     clothing         clothing     None       BrandA  470.75     179.20  256.21   2.61   33x34x35  Black  None             261          488             12      SUP_01         None  2023-02-13T18:30:00.000Z     false   None\n",
      "\n",
      "Data Types:\n",
      "product_id          object\n",
      "item_id              int64\n",
      "product_name        object\n",
      "item_name           object\n",
      "description         object\n",
      "category            object\n",
      "product_category    object\n",
      "brand               object\n",
      "manufacturer        object\n",
      "price               object\n",
      "list_price          object\n",
      "cost                object\n",
      "weight              object\n",
      "dimensions          object\n",
      "color               object\n",
      "size                object\n",
      "stock_quantity       int64\n",
      "stock_level          int64\n",
      "reorder_level        int64\n",
      "supplier_id         object\n",
      "created_date        object\n",
      "last_updated        object\n",
      "is_active           object\n",
      "rating              object\n",
      "dtype: object\n",
      "\n",
      "Missing values (count and percentage):\n",
      "              count  percentage\n",
      "description      55        27.5\n",
      "brand            29        14.5\n",
      "manufacturer     27        13.5\n",
      "weight           77        38.5\n",
      "color            30        15.0\n",
      "size             30        15.0\n",
      "supplier_id      47        23.5\n",
      "created_date     29        14.5\n",
      "rating           62        31.0\n",
      "\n",
      "Duplicate rows (entire row): 0\n",
      "\n",
      "Descriptive statistics (numerical columns - attempt conversion for relevant ones):\n",
      "            price  list_price        cost      weight  stock_quantity  stock_level  reorder_level      rating\n",
      "count  200.000000  200.000000  200.000000  123.000000      200.000000   200.000000     200.000000  138.000000\n",
      "mean   273.694650  323.472400  166.180500    5.406260      495.355000   257.935000      23.545000    2.416667\n",
      "std    147.912127  184.226269   89.096967    2.974296      286.989309   143.678734      14.398422    1.500831\n",
      "min     10.580000   15.400000    6.450000    0.230000        0.000000     3.000000       0.000000    0.000000\n",
      "25%    150.032500  158.720000   88.892500    3.070000      228.500000   128.250000      11.000000    1.225000\n",
      "50%    287.835000  334.815000  170.795000    5.550000      508.000000   264.500000      22.000000    2.100000\n",
      "75%    405.852500  500.167500  243.212500    8.250000      758.500000   381.250000      35.000000    3.775000\n",
      "max    508.730000  611.330000  302.610000   10.000000      998.000000   499.000000      49.000000    5.000000\n",
      "\n",
      "Descriptive statistics (object/categorical columns):\n",
      "       product_id product_name    item_name                description     category product_category    brand manufacturer   price list_price   cost weight dimensions color      size supplier_id created_date last_updated is_active rating\n",
      "count         200          200          200                        145          200              200      171          173     200        200    200    123        200   170       170         153          171          200       200    138\n",
      "unique        200          200          200                        145            8                8        6            6     199        199    200    114        120     6         6          20          166          137         6     48\n",
      "top      PROD_001    Product 1  Item 1 Name  Description for product 1  Electronics         clothing  BRAND-C      BRAND-C  447.55      25.72  61.01   3.59              Red  One Size      SUP_05   2021-09-27                   True    1.4\n",
      "freq            1            1            1                          1           35               32       39           32       2          2      1      2         80    40        35          13            2           17        49      6\n",
      "\n",
      "Value counts for 'product_id':\n",
      "product_id\n",
      "PROD_001    1\n",
      "PROD_002    1\n",
      "PROD_003    1\n",
      "PROD_004    1\n",
      "PROD_005    1\n",
      "PROD_006    1\n",
      "PROD_007    1\n",
      "PROD_008    1\n",
      "PROD_009    1\n",
      "PROD_010    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'product_id': 200\n",
      "\n",
      "Value counts for 'item_id':\n",
      "item_id\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'item_id': 200\n",
      "\n",
      "Value counts for 'product_name':\n",
      "product_name\n",
      "Product 1     1\n",
      "Product 2     1\n",
      "Product 3     1\n",
      "Product 4     1\n",
      "Product 5     1\n",
      "Product 6     1\n",
      "Product 7     1\n",
      "Product 8     1\n",
      "Product 9     1\n",
      "Product 10    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'product_name': 200\n",
      "\n",
      "Value counts for 'item_name':\n",
      "item_name\n",
      "Item 1 Name     1\n",
      "Item 2 Name     1\n",
      "Item 3 Name     1\n",
      "Item 4 Name     1\n",
      "Item 5 Name     1\n",
      "Item 6 Name     1\n",
      "Item 7 Name     1\n",
      "Item 8 Name     1\n",
      "Item 9 Name     1\n",
      "Item 10 Name    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'item_name': 200\n",
      "\n",
      "Value counts for 'category':\n",
      "category\n",
      "Electronics      35\n",
      "electronics      30\n",
      "Sports           27\n",
      "clothing         24\n",
      "Toys             24\n",
      "CLOTHING         21\n",
      "Home & Garden    20\n",
      "Books            19\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'category': 8\n",
      "\n",
      "Value counts for 'product_category':\n",
      "product_category\n",
      "clothing         32\n",
      "Sports           32\n",
      "Toys             25\n",
      "electronics      24\n",
      "Electronics      23\n",
      "Books            23\n",
      "CLOTHING         21\n",
      "Home & Garden    20\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'product_category': 8\n",
      "\n",
      "Value counts for 'brand':\n",
      "brand\n",
      "BRAND-C    39\n",
      "brand_b    34\n",
      "Brand D    30\n",
      "None       29\n",
      "BrandA     25\n",
      "brandE     23\n",
      "           20\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'brand': 7\n",
      "\n",
      "Value counts for 'manufacturer':\n",
      "manufacturer\n",
      "BRAND-C    32\n",
      "BrandA     31\n",
      "           30\n",
      "brand_b    29\n",
      "None       27\n",
      "Brand D    26\n",
      "brandE     25\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'manufacturer': 7\n",
      "\n",
      "Value counts for 'supplier_id':\n",
      "supplier_id\n",
      "None      47\n",
      "SUP_05    13\n",
      "SUP_17    11\n",
      "SUP_03    10\n",
      "SUP_01    10\n",
      "SUP_12     9\n",
      "SUP_16     8\n",
      "SUP_11     8\n",
      "SUP_14     8\n",
      "SUP_04     8\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'supplier_id': 21\n",
      "\n",
      "Value counts for 'is_active':\n",
      "is_active\n",
      "True     49\n",
      "False    47\n",
      "true     33\n",
      "yes      29\n",
      "no       21\n",
      "false    21\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'is_active': 6\n",
      "\n",
      "Value counts for 'color':\n",
      "color\n",
      "Red      40\n",
      "Blue     30\n",
      "None     30\n",
      "Black    29\n",
      "White    27\n",
      "         23\n",
      "Green    21\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'color': 7\n",
      "\n",
      "Value counts for 'size':\n",
      "size\n",
      "One Size    35\n",
      "M           30\n",
      "None        30\n",
      "S           27\n",
      "            27\n",
      "L           26\n",
      "XL          25\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'size': 7\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Exploring Raw Product Data (products_inconsistent_data.json) ---\")\n",
    "if not df_products_raw.empty:\n",
    "    print(\"Shape:\", df_products_raw.shape)\n",
    "    \n",
    "    print(\"\\nInfo:\")\n",
    "    df_products_raw.info()\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_products_raw.head())\n",
    "\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_products_raw.tail())\n",
    "        \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df_products_raw.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values (count and percentage):\")\n",
    "    missing_values = df_products_raw.isnull().sum()\n",
    "    missing_percentage = (df_products_raw.isnull().sum() / len(df_products_raw)) * 100\n",
    "    missing_df = pd.DataFrame({'count': missing_values, 'percentage': missing_percentage})\n",
    "    print(missing_df[missing_df['count'] > 0])\n",
    "    \n",
    "    print(\"\\nDuplicate rows (entire row):\", df_products_raw.duplicated().sum())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (numerical columns - attempt conversion for relevant ones):\")\n",
    "    # Attempt to convert price-like columns for describe, but be cautious with errors\n",
    "    numeric_potential_cols = ['price', 'list_price', 'cost', 'weight', 'stock_quantity', 'stock_level', 'reorder_level', 'rating']\n",
    "    df_products_numeric_describe = df_products_raw.copy()\n",
    "    for col in numeric_potential_cols:\n",
    "        if col in df_products_numeric_describe.columns:\n",
    "            df_products_numeric_describe[col] = pd.to_numeric(df_products_numeric_describe[col], errors='coerce')\n",
    "    print(df_products_numeric_describe[numeric_potential_cols].describe())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (object/categorical columns):\")\n",
    "    print(df_products_raw.describe(include=['object']))\n",
    "\n",
    "    logger.info(\"Value counts for key categorical/ID columns in Products:\")\n",
    "    key_product_cols = ['product_id', 'item_id', 'product_name', 'item_name', 'category', \n",
    "                        'product_category', 'brand', 'manufacturer', 'supplier_id', 'is_active', 'color', 'size']\n",
    "    for col in key_product_cols:\n",
    "        if col in df_products_raw.columns:\n",
    "            print(f\"\\nValue counts for '{col}':\")\n",
    "            print(df_products_raw[col].value_counts(dropna=False).head(10))\n",
    "            print(f\"Number of unique values in '{col}': {df_products_raw[col].nunique(dropna=False)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Column '{col}' not found in raw product data.\")\n",
    "            \n",
    "    logger.info(\"Product data exploration complete.\")\n",
    "else:\n",
    "    logger.warning(\"Product DataFrame (df_products_raw) is empty. Skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180f2e1-92b0-43a3-adeb-02f558b62632",
   "metadata": {},
   "source": [
    "# Exploration for df_reconciliation_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48a930e-4dea-4c06-a359-a8904ad1e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,253 - INFO - \n",
      "--- Exploring Raw Reconciliation Data (reconciliation_challenge_data.csv) ---\n",
      "2025-06-23 08:48:42,328 - INFO - Value counts for key categorical/ID columns in Reconciliation Data:\n",
      "2025-06-23 08:48:42,342 - INFO - Reconciliation data exploration complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (300, 20)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 20 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   client_reference         300 non-null    object \n",
      " 1   full_customer_name       300 non-null    object \n",
      " 2   contact_email            300 non-null    object \n",
      " 3   transaction_ref          300 non-null    object \n",
      " 4   item_reference           300 non-null    object \n",
      " 5   transaction_date         300 non-null    object \n",
      " 6   amount_paid              300 non-null    float64\n",
      " 7   payment_status           300 non-null    object \n",
      " 8   delivery_status          300 non-null    object \n",
      " 9   customer_segment         300 non-null    object \n",
      " 10  region                   300 non-null    object \n",
      " 11  product_line             300 non-null    object \n",
      " 12  quantity_ordered         300 non-null    int64  \n",
      " 13  unit_cost                300 non-null    float64\n",
      " 14  total_value              300 non-null    float64\n",
      " 15  discount_applied         300 non-null    float64\n",
      " 16  shipping_fee             300 non-null    float64\n",
      " 17  tax_amount               300 non-null    float64\n",
      " 18  notes_comments           97 non-null     object \n",
      " 19  last_modified_timestamp  300 non-null    object \n",
      "dtypes: float64(6), int64(1), object(13)\n",
      "memory usage: 47.0+ KB\n",
      "\n",
      "First 5 rows:\n",
      "  client_reference full_customer_name        contact_email transaction_ref item_reference transaction_date  amount_paid payment_status delivery_status customer_segment region product_line  quantity_ordered  unit_cost  total_value  discount_applied  shipping_fee  tax_amount notes_comments   last_modified_timestamp\n",
      "0         CLI_0280         John Smith  reconcile1@test.com       TXN_00119        ITM_076        2/12/2023       664.50         FAILED       DELIVERED          PREMIUM  SOUTH         HOME                 2     159.48       339.12             68.82          7.84       16.62            NaN  2023-12-26T18:30:00.000Z\n",
      "1         CLI_0360           Jane Doe  reconcile2@test.com       TXN_00522        ITM_015        3/14/2023       897.23        PENDING         PENDING              VIP  NORTH  ELECTRONICS                 6     214.26       776.90             96.22         28.67       32.03            NaN  2023-12-13T18:30:00.000Z\n",
      "2         CLI_0449         Bob Wilson  reconcile3@test.com       TXN_00733        ITM_167        6/24/2023       252.18         FAILED       DELIVERED         STANDARD   WEST        BOOKS                 7     296.42       425.96             55.15          5.67       62.67            NaN  2023-09-17T18:30:00.000Z\n",
      "3         CLI_0071         Bob Wilson  reconcile4@test.com       TXN_00014        ITM_142        7/23/2023       255.92         FAILED      IN_TRANSIT         STANDARD   EAST      FASHION                 8     244.51       226.61             61.96         19.46       35.95            NaN  2023-03-03T18:30:00.000Z\n",
      "4         CLI_0305           Jane Doe  reconcile5@test.com       TXN_00482        ITM_105       12/22/2023       897.18      COMPLETED      IN_TRANSIT              VIP  SOUTH      FASHION                 5     108.60       471.31             98.82         16.83       52.79            NaN  2023-06-27T18:30:00.000Z\n",
      "\n",
      "Last 5 rows:\n",
      "    client_reference full_customer_name          contact_email transaction_ref item_reference transaction_date  amount_paid payment_status delivery_status customer_segment region product_line  quantity_ordered  unit_cost  total_value  discount_applied  shipping_fee  tax_amount                  notes_comments   last_modified_timestamp\n",
      "295         CLI_0297         John Smith  reconcile296@test.com       TXN_00755        ITM_051        8/23/2023       516.07        PENDING       DELIVERED         STANDARD   WEST         HOME                10      42.47       543.89             46.89         21.66       48.19                             NaN  2023-09-25T18:30:00.000Z\n",
      "296         CLI_0073         John Smith  reconcile297@test.com       TXN_00209        ITM_087        7/12/2023       552.58        PENDING         PENDING         STANDARD  NORTH      FASHION                 5     270.88       642.53             75.04         10.66       20.29                             NaN  2023-07-17T18:30:00.000Z\n",
      "297         CLI_0041         Bob Wilson  reconcile298@test.com       TXN_00706        ITM_120         7/8/2023       308.65         FAILED       DELIVERED          PREMIUM  NORTH      FASHION                 6     228.03       406.34             38.38          5.35       31.01                             NaN  2023-09-06T18:30:00.000Z\n",
      "298         CLI_0354         John Smith  reconcile299@test.com       TXN_00627        ITM_191       12/15/2023       547.85        PENDING         PENDING         STANDARD   EAST         HOME                 8      78.61       279.87             44.62         25.63       14.90                             NaN  2023-11-27T18:30:00.000Z\n",
      "299         CLI_0270           Jane Doe  reconcile300@test.com       TXN_00832        ITM_043         7/5/2023       763.93      COMPLETED      IN_TRANSIT         STANDARD  SOUTH         HOME                10     202.93       608.67             10.75          9.50       52.28  Additional processing required  2023-05-03T18:30:00.000Z\n",
      "\n",
      "Data Types:\n",
      "client_reference            object\n",
      "full_customer_name          object\n",
      "contact_email               object\n",
      "transaction_ref             object\n",
      "item_reference              object\n",
      "transaction_date            object\n",
      "amount_paid                float64\n",
      "payment_status              object\n",
      "delivery_status             object\n",
      "customer_segment            object\n",
      "region                      object\n",
      "product_line                object\n",
      "quantity_ordered             int64\n",
      "unit_cost                  float64\n",
      "total_value                float64\n",
      "discount_applied           float64\n",
      "shipping_fee               float64\n",
      "tax_amount                 float64\n",
      "notes_comments              object\n",
      "last_modified_timestamp     object\n",
      "dtype: object\n",
      "\n",
      "Missing values (count and percentage):\n",
      "                count  percentage\n",
      "notes_comments    203   67.666667\n",
      "\n",
      "Duplicate rows (entire row): 0\n",
      "\n",
      "Descriptive statistics (numerical columns):\n",
      "       amount_paid  quantity_ordered   unit_cost  total_value  discount_applied  shipping_fee  tax_amount\n",
      "count   300.000000        300.000000  300.000000   300.000000        300.000000    300.000000  300.000000\n",
      "mean    513.555100          5.506667  160.827567   516.105200         50.538367     17.905300   34.223433\n",
      "std     296.454711          2.817164   89.042084   220.495963         29.148140      7.148195   16.360917\n",
      "min      50.340000          1.000000   10.320000   102.470000          0.230000      5.350000    5.050000\n",
      "25%     258.265000          3.000000   81.570000   337.902500         24.952500     12.227500   19.922500\n",
      "50%     470.360000          6.000000  157.390000   515.275000         50.035000     18.300000   34.775000\n",
      "75%     768.147500          8.000000  237.770000   702.095000         77.217500     24.197500   47.207500\n",
      "max    1048.640000         10.000000  309.140000   896.530000         99.760000     29.930000   64.800000\n",
      "\n",
      "Descriptive statistics (object/categorical columns):\n",
      "       client_reference full_customer_name        contact_email transaction_ref item_reference transaction_date payment_status delivery_status customer_segment region product_line                  notes_comments   last_modified_timestamp\n",
      "count               300                300                  300             300            300              300            300             300              300    300          300                              97                       300\n",
      "unique              222                  4                  300             265            160              201              3               3                3      4            4                               1                       198\n",
      "top            CLI_0476         Bob Wilson  reconcile1@test.com       TXN_00490        ITM_076        6/24/2023        PENDING      IN_TRANSIT         STANDARD   EAST         HOME  Additional processing required  2023-12-20T18:30:00.000Z\n",
      "freq                  4                 81                    1               3              5                4            110             106              113     79           85                              97                         7\n",
      "\n",
      "Value counts for 'client_reference':\n",
      "client_reference\n",
      "CLI_0476    4\n",
      "CLI_0495    4\n",
      "CLI_0359    3\n",
      "CLI_0003    3\n",
      "CLI_0218    3\n",
      "CLI_0071    3\n",
      "CLI_0004    3\n",
      "CLI_0249    3\n",
      "CLI_0356    3\n",
      "CLI_0192    3\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'client_reference': 222\n",
      "\n",
      "Value counts for 'transaction_ref':\n",
      "transaction_ref\n",
      "TXN_00490    3\n",
      "TXN_00179    3\n",
      "TXN_00841    2\n",
      "TXN_00237    2\n",
      "TXN_00190    2\n",
      "TXN_00033    2\n",
      "TXN_00347    2\n",
      "TXN_00676    2\n",
      "TXN_00632    2\n",
      "TXN_00805    2\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'transaction_ref': 265\n",
      "\n",
      "Value counts for 'item_reference':\n",
      "item_reference\n",
      "ITM_076    5\n",
      "ITM_200    5\n",
      "ITM_192    5\n",
      "ITM_004    4\n",
      "ITM_154    4\n",
      "ITM_169    4\n",
      "ITM_137    4\n",
      "ITM_189    4\n",
      "ITM_148    4\n",
      "ITM_003    4\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'item_reference': 160\n",
      "\n",
      "Value counts for 'payment_status':\n",
      "payment_status\n",
      "PENDING      110\n",
      "COMPLETED     96\n",
      "FAILED        94\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'payment_status': 3\n",
      "\n",
      "Value counts for 'delivery_status':\n",
      "delivery_status\n",
      "IN_TRANSIT    106\n",
      "DELIVERED      97\n",
      "PENDING        97\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'delivery_status': 3\n",
      "\n",
      "Value counts for 'customer_segment':\n",
      "customer_segment\n",
      "STANDARD    113\n",
      "PREMIUM      94\n",
      "VIP          93\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'customer_segment': 3\n",
      "\n",
      "Value counts for 'region':\n",
      "region\n",
      "EAST     79\n",
      "SOUTH    77\n",
      "WEST     74\n",
      "NORTH    70\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'region': 4\n",
      "\n",
      "Value counts for 'product_line':\n",
      "product_line\n",
      "HOME           85\n",
      "BOOKS          74\n",
      "ELECTRONICS    71\n",
      "FASHION        70\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'product_line': 4\n",
      "\n",
      "Number of rows where total_value != quantity * unit_cost: 300\n",
      "Examples of discrepancies:\n",
      "   quantity_ordered  unit_cost  total_value  calculated_total\n",
      "0                 2     159.48       339.12            318.96\n",
      "1                 6     214.26       776.90           1285.56\n",
      "2                 7     296.42       425.96           2074.94\n",
      "3                 8     244.51       226.61           1956.08\n",
      "4                 5     108.60       471.31            543.00\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Exploring Raw Reconciliation Data (reconciliation_challenge_data.csv) ---\")\n",
    "if not df_reconciliation_raw.empty:\n",
    "    print(\"Shape:\", df_reconciliation_raw.shape)\n",
    "    \n",
    "    print(\"\\nInfo:\")\n",
    "    df_reconciliation_raw.info()\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_reconciliation_raw.head())\n",
    "\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_reconciliation_raw.tail())\n",
    "        \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df_reconciliation_raw.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values (count and percentage):\")\n",
    "    missing_values = df_reconciliation_raw.isnull().sum()\n",
    "    missing_percentage = (df_reconciliation_raw.isnull().sum() / len(df_reconciliation_raw)) * 100\n",
    "    missing_df = pd.DataFrame({'count': missing_values, 'percentage': missing_percentage})\n",
    "    print(missing_df[missing_df['count'] > 0])\n",
    "    \n",
    "    print(\"\\nDuplicate rows (entire row):\", df_reconciliation_raw.duplicated().sum())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (numerical columns):\")\n",
    "    # Select columns that are expected to be numeric\n",
    "    numeric_cols_recon = ['amount_paid', 'quantity_ordered', 'unit_cost', 'total_value', \n",
    "                          'discount_applied', 'shipping_fee', 'tax_amount']\n",
    "    print(df_reconciliation_raw[numeric_cols_recon].describe())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (object/categorical columns):\")\n",
    "    print(df_reconciliation_raw.describe(include=['object']))\n",
    "\n",
    "    logger.info(\"Value counts for key categorical/ID columns in Reconciliation Data:\")\n",
    "    key_recon_cols = ['client_reference', 'transaction_ref', 'item_reference', 'payment_status', \n",
    "                      'delivery_status', 'customer_segment', 'region', 'product_line']\n",
    "    for col in key_recon_cols:\n",
    "        if col in df_reconciliation_raw.columns:\n",
    "            print(f\"\\nValue counts for '{col}':\")\n",
    "            print(df_reconciliation_raw[col].value_counts(dropna=False).head(10))\n",
    "            print(f\"Number of unique values in '{col}': {df_reconciliation_raw[col].nunique(dropna=False)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Column '{col}' not found in raw reconciliation data.\")\n",
    "            \n",
    "    # Check financial integrity: total_value vs (quantity * unit_cost)\n",
    "    df_reconciliation_raw['calculated_total'] = df_reconciliation_raw['quantity_ordered'] * df_reconciliation_raw['unit_cost']\n",
    "    discrepancies = df_reconciliation_raw[\n",
    "        ~np.isclose(df_reconciliation_raw['total_value'].fillna(0), df_reconciliation_raw['calculated_total'].fillna(0))\n",
    "    ]\n",
    "    print(f\"\\nNumber of rows where total_value != quantity * unit_cost: {len(discrepancies)}\")\n",
    "    if not discrepancies.empty:\n",
    "        print(\"Examples of discrepancies:\")\n",
    "        print(discrepancies[['quantity_ordered', 'unit_cost', 'total_value', 'calculated_total']].head())\n",
    "    df_reconciliation_raw.drop(columns=['calculated_total'], inplace=True, errors='ignore')\n",
    "\n",
    "    logger.info(\"Reconciliation data exploration complete.\")\n",
    "else:\n",
    "    logger.warning(\"Reconciliation DataFrame (df_reconciliation_raw) is empty. Skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a12a4-4181-4db8-bd9f-912d70f7215a",
   "metadata": {},
   "source": [
    "# Exploration for df_orders_unstructured_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac909366-c8db-4940-a727-42a346638399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,351 - INFO - \n",
      "--- Exploring Raw Orders Unstructured Data (orders_unstructured_data.csv) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 23)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   order_id          1000 non-null   object \n",
      " 1   ord_id            1000 non-null   int64  \n",
      " 2   customer_id       1000 non-null   int64  \n",
      " 3   cust_id           1000 non-null   object \n",
      " 4   order_date        914 non-null    object \n",
      " 5   order_datetime    795 non-null    object \n",
      " 6   product_id        1000 non-null   object \n",
      " 7   item_id           1000 non-null   int64  \n",
      " 8   quantity          1000 non-null   int64  \n",
      " 9   qty               1000 non-null   int64  \n",
      " 10  unit_price        1000 non-null   float64\n",
      " 11  price             1000 non-null   float64\n",
      " 12  total_amount      1000 non-null   float64\n",
      " 13  order_total       1000 non-null   float64\n",
      " 14  shipping_cost     1000 non-null   float64\n",
      " 15  tax               1000 non-null   float64\n",
      " 16  discount          1000 non-null   float64\n",
      " 17  status            863 non-null    object \n",
      " 18  order_status      838 non-null    object \n",
      " 19  payment_method    1000 non-null   object \n",
      " 20  shipping_address  1000 non-null   object \n",
      " 21  notes             214 non-null    object \n",
      " 22  tracking_number   594 non-null    object \n",
      "dtypes: float64(7), int64(5), object(11)\n",
      "memory usage: 179.8+ KB\n",
      "\n",
      "First 5 rows:\n",
      "    order_id  ord_id  customer_id    cust_id  order_date            order_datetime product_id  item_id  quantity  qty  unit_price   price  total_amount  order_total  shipping_cost    tax  discount      status order_status payment_method shipping_address                          notes tracking_number\n",
      "0  ORD_00001       1          295  CUST_0295  2023-10-21  2023-06-01T00:39:00.000Z   PROD_140      116         6    4      381.29  101.67        774.00       349.25           6.90  50.40      9.27  processing      SHIPPED     debit_card     4136 Main St  Special delivery instructions             NaN\n",
      "1  ORD_00002       2           87  CUST_0087   12/4/2023  2023-11-22T17:08:00.000Z   PROD_033       38         8    4      153.77  100.11        152.92        49.45          16.45  27.77     36.07   delivered          NaN  bank_transfer     3133 Oak Ave                            NaN             NaN\n",
      "2  ORD_00003       3          297  CUST_0297   11/2/2023  2023-01-11T15:58:00.000Z   PROD_172       27         1    3       94.72  167.06        583.22       657.02           9.89   3.94      0.00  processing    CANCELLED         paypal     7377 Main St                            NaN             NaN\n",
      "3  ORD_00004       4           18  CUST_0018  2023-10-03  2023-09-22T00:17:00.000Z   PROD_006       17         1    1       76.03  286.08        339.32       545.57           6.92   4.51      0.00   delivered      SHIPPED     debit_card     9595 Main St                            NaN             NaN\n",
      "4  ORD_00005       5           35  CUST_0035   12/8/2023  2023-07-23T14:03:00.000Z   PROD_010      138         4    2      287.33   75.69       1036.75       192.12           0.00  18.43      0.00   CANCELLED      pending         paypal     9417 Oak Ave                            NaN       TRK258913\n",
      "\n",
      "Last 5 rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,460 - INFO - Value counts for key categorical/ID columns in Orders Unstructured Data:\n",
      "2025-06-23 08:48:42,470 - INFO - Orders Unstructured data exploration complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      order_id  ord_id  customer_id    cust_id  order_date            order_datetime product_id  item_id  quantity  qty  unit_price   price  total_amount  order_total  shipping_cost    tax  discount      status order_status payment_method shipping_address                          notes tracking_number\n",
      "995  ORD_00996     996          139  CUST_0139  2023-10-04  2023-01-21T21:18:00.000Z   PROD_198      155         5    4      108.40   43.36        705.49       535.21           0.00  26.67      0.00    returned          NaN  bank_transfer     7790 Main St                            NaN       TRK520098\n",
      "996  ORD_00997     997          233  CUST_0233  2023-11-06  2023-07-04T06:17:00.000Z   PROD_173      146         5    5       58.79  223.80        990.20       822.90          23.90  29.42      0.00   delivered      pending    credit_card     6580 Oak Ave                            NaN             NaN\n",
      "997  ORD_00998     998          241  CUST_0241    7/9/2023  2023-03-26T03:58:00.000Z   PROD_054      167         4    4      339.42  238.22        442.99        93.12          18.88  25.52      0.00     pending          NaN  bank_transfer     8298 Oak Ave  Special delivery instructions             NaN\n",
      "998  ORD_00999     999          245  CUST_0245   8/20/2023                       NaN   PROD_081      198         5    2      415.19  392.10        857.34       540.12           0.00  27.05     12.09  processing      SHIPPED         paypal     8233 Oak Ave  Special delivery instructions             NaN\n",
      "999  ORD_01000    1000          301  CUST_0301  2023-10-24                       NaN   PROD_150       41         2    2      101.45  192.93        396.32       303.79          20.95  18.48      0.00     pending     returned    credit_card     9481 Oak Ave                            NaN             NaN\n",
      "\n",
      "Data Types:\n",
      "order_id             object\n",
      "ord_id                int64\n",
      "customer_id           int64\n",
      "cust_id              object\n",
      "order_date           object\n",
      "order_datetime       object\n",
      "product_id           object\n",
      "item_id               int64\n",
      "quantity              int64\n",
      "qty                   int64\n",
      "unit_price          float64\n",
      "price               float64\n",
      "total_amount        float64\n",
      "order_total         float64\n",
      "shipping_cost       float64\n",
      "tax                 float64\n",
      "discount            float64\n",
      "status               object\n",
      "order_status         object\n",
      "payment_method       object\n",
      "shipping_address     object\n",
      "notes                object\n",
      "tracking_number      object\n",
      "dtype: object\n",
      "\n",
      "Missing values (count and percentage):\n",
      "                 count  percentage\n",
      "order_date          86         8.6\n",
      "order_datetime     205        20.5\n",
      "status             137        13.7\n",
      "order_status       162        16.2\n",
      "notes              786        78.6\n",
      "tracking_number    406        40.6\n",
      "\n",
      "Duplicate rows (entire row): 0\n",
      "\n",
      "Descriptive statistics (numerical columns - attempt conversion):\n",
      "          quantity          qty   unit_price        price  total_amount  order_total  shipping_cost          tax    discount\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000   1000.000000  1000.000000    1000.000000  1000.000000  1000.00000\n",
      "mean      5.435000     2.947000   251.587030   214.525270    542.310730   443.776710      10.414570    26.713270     7.53685\n",
      "std       2.870194     1.409318   142.940666   113.267041    291.622739   236.005628       8.148008    14.811964    13.76221\n",
      "min       1.000000     1.000000    10.020000    15.000000     51.320000    41.230000       0.000000     2.030000     0.00000\n",
      "25%       3.000000     2.000000   125.695000   116.405000    293.647500   239.830000       0.000000    13.517500     0.00000\n",
      "50%       5.000000     3.000000   245.330000   219.345000    542.410000   436.565000      10.570000    26.440000     0.00000\n",
      "75%       8.000000     4.000000   376.505000   315.140000    788.992500   653.355000      17.225000    39.885000     9.22500\n",
      "max      10.000000     5.000000   509.800000   413.980000   1049.310000   839.060000      24.950000    51.950000    49.55000\n",
      "\n",
      "Descriptive statistics (object/categorical columns):\n",
      "         order_id    cust_id  order_date            order_datetime product_id     status order_status payment_method shipping_address                          notes tracking_number\n",
      "count        1000       1000         914                       795       1000        863          838           1000             1000                            214             594\n",
      "unique       1000        431         495                       793        199          6            6              5              986                              1             594\n",
      "top     ORD_00001  CUST_0163  2023-01-06  2023-01-25T01:00:00.000Z   PROD_180  CANCELLED   processing         paypal     9023 Main St  Special delivery instructions       TRK258913\n",
      "freq            1          8           5                         2         12        149          159            209                3                            214               1\n",
      "\n",
      "Value counts for 'order_id':\n",
      "order_id\n",
      "ORD_00001    1\n",
      "ORD_00002    1\n",
      "ORD_00003    1\n",
      "ORD_00004    1\n",
      "ORD_00005    1\n",
      "ORD_00006    1\n",
      "ORD_00007    1\n",
      "ORD_00008    1\n",
      "ORD_00009    1\n",
      "ORD_00010    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'order_id': 1000\n",
      "\n",
      "Value counts for 'ord_id':\n",
      "ord_id\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'ord_id': 1000\n",
      "\n",
      "Value counts for 'customer_id':\n",
      "customer_id\n",
      "163    8\n",
      "96     7\n",
      "35     7\n",
      "357    7\n",
      "175    6\n",
      "57     6\n",
      "397    6\n",
      "177    6\n",
      "408    6\n",
      "218    6\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'customer_id': 431\n",
      "\n",
      "Value counts for 'cust_id':\n",
      "cust_id\n",
      "CUST_0163    8\n",
      "CUST_0096    7\n",
      "CUST_0035    7\n",
      "CUST_0357    7\n",
      "CUST_0175    6\n",
      "CUST_0057    6\n",
      "CUST_0397    6\n",
      "CUST_0177    6\n",
      "CUST_0408    6\n",
      "CUST_0218    6\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'cust_id': 431\n",
      "\n",
      "Value counts for 'product_id':\n",
      "product_id\n",
      "PROD_180    12\n",
      "PROD_185    10\n",
      "PROD_032    10\n",
      "PROD_169    10\n",
      "PROD_029     9\n",
      "PROD_064     9\n",
      "PROD_052     9\n",
      "PROD_155     9\n",
      "PROD_178     9\n",
      "PROD_113     9\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'product_id': 199\n",
      "\n",
      "Value counts for 'item_id':\n",
      "item_id\n",
      "140    12\n",
      "46     10\n",
      "171    10\n",
      "153    10\n",
      "186    10\n",
      "178    10\n",
      "25     10\n",
      "166    10\n",
      "120     9\n",
      "138     9\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'item_id': 199\n",
      "\n",
      "Value counts for 'status':\n",
      "status\n",
      "CANCELLED     149\n",
      "pending       147\n",
      "returned      145\n",
      "processing    143\n",
      "delivered     141\n",
      "SHIPPED       138\n",
      "NaN           137\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'status': 7\n",
      "\n",
      "Value counts for 'order_status':\n",
      "order_status\n",
      "NaN           162\n",
      "processing    159\n",
      "returned      148\n",
      "delivered     140\n",
      "SHIPPED       138\n",
      "pending       133\n",
      "CANCELLED     120\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'order_status': 7\n",
      "\n",
      "Value counts for 'payment_method':\n",
      "payment_method\n",
      "paypal           209\n",
      "cash             203\n",
      "debit_card       202\n",
      "bank_transfer    201\n",
      "credit_card      185\n",
      "Name: count, dtype: int64\n",
      "Number of unique values in 'payment_method': 5\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Exploring Raw Orders Unstructured Data (orders_unstructured_data.csv) ---\")\n",
    "if not df_orders_unstructured_raw.empty:\n",
    "    print(\"Shape:\", df_orders_unstructured_raw.shape)\n",
    "    \n",
    "    print(\"\\nInfo:\")\n",
    "    df_orders_unstructured_raw.info()\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_orders_unstructured_raw.head())\n",
    "\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_orders_unstructured_raw.tail())\n",
    "        \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df_orders_unstructured_raw.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values (count and percentage):\")\n",
    "    missing_values = df_orders_unstructured_raw.isnull().sum()\n",
    "    missing_percentage = (df_orders_unstructured_raw.isnull().sum() / len(df_orders_unstructured_raw)) * 100\n",
    "    missing_df = pd.DataFrame({'count': missing_values, 'percentage': missing_percentage})\n",
    "    print(missing_df[missing_df['count'] > 0])\n",
    "    \n",
    "    print(\"\\nDuplicate rows (entire row):\", df_orders_unstructured_raw.duplicated().sum())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (numerical columns - attempt conversion):\")\n",
    "    numeric_potential_cols_orders = ['quantity', 'qty', 'unit_price', 'price', 'total_amount', \n",
    "                                     'order_total', 'shipping_cost', 'tax', 'discount']\n",
    "    df_orders_numeric_describe = df_orders_unstructured_raw.copy()\n",
    "    for col in numeric_potential_cols_orders:\n",
    "        if col in df_orders_numeric_describe.columns:\n",
    "            df_orders_numeric_describe[col] = pd.to_numeric(df_orders_numeric_describe[col], errors='coerce')\n",
    "    print(df_orders_numeric_describe[numeric_potential_cols_orders].describe())\n",
    "    \n",
    "    print(\"\\nDescriptive statistics (object/categorical columns):\")\n",
    "    print(df_orders_unstructured_raw.describe(include=['object']))\n",
    "\n",
    "    logger.info(\"Value counts for key categorical/ID columns in Orders Unstructured Data:\")\n",
    "    key_orders_cols = ['order_id', 'ord_id', 'customer_id', 'cust_id', 'product_id', 'item_id',\n",
    "                       'status', 'order_status', 'payment_method']\n",
    "    for col in key_orders_cols:\n",
    "        if col in df_orders_unstructured_raw.columns:\n",
    "            print(f\"\\nValue counts for '{col}':\")\n",
    "            print(df_orders_unstructured_raw[col].value_counts(dropna=False).head(10))\n",
    "            print(f\"Number of unique values in '{col}': {df_orders_unstructured_raw[col].nunique(dropna=False)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Column '{col}' not found in raw orders unstructured data.\")\n",
    "            \n",
    "    logger.info(\"Orders Unstructured data exploration complete.\")\n",
    "else:\n",
    "    logger.warning(\"Orders Unstructured DataFrame (df_orders_unstructured_raw) is empty. Skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff11d4-80bd-4588-8d5e-46aa4d3076b9",
   "metadata": {},
   "source": [
    "# Detailed Data Quality Issues & Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1713743-a3a4-458a-aa20-af7e47fae916",
   "metadata": {},
   "source": [
    "# 1. customers_messy_data.json (Customer Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662c906-634c-44f0-83cf-b9b1c92e3688",
   "metadata": {},
   "source": [
    "# Simple Summary of Customer Data Issues (customers_messy_data.json):\n",
    "\n",
    "- Main ID: Use cust_id as the main customer ID  it's clean and consistent.\n",
    "\n",
    "- Names: Some are proper names, others are emails or usernames  pick the best and format nicely.\n",
    "\n",
    "- Emails: Two email fields  combine them, keep valid ones, make lowercase.\n",
    "\n",
    "- Phone Numbers: Messy formats  merge and clean them up to one standard format.\n",
    "\n",
    "- Addresses: Clean up street, city, state, and zip codes; make them consistent.\n",
    "\n",
    "- Dates: Dates are in different formats  combine and convert to a standard one.\n",
    "\n",
    "- Status Info: Two fields for status  merge and standardize (like ACTIVE or INACTIVE).\n",
    "\n",
    "- Spending & Age: Fix number formats and make sure age is a proper whole number.\n",
    "\n",
    "- Other Info: Standardize things like gender, payment method, and customer segment.\n",
    "\n",
    "- Duplicates: No exact copy-paste duplicates, but similar entries might exist  check after cleaning.\n",
    "\n",
    "- Linking: cust_id will be used to connect with orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc6329-c129-4e49-a76f-1e113298bcc0",
   "metadata": {},
   "source": [
    "# 2. products_inconsistent_data.json (Product Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337c668-0ab8-4af4-856c-b3133463c088",
   "metadata": {},
   "source": [
    "# Simple Summary of Product Data Issues (products_inconsistent_data.json):\n",
    "\n",
    "- Main ID: Use product_id (like PROD_001) as the main product ID. Keep item_id for reference.\n",
    "\n",
    "- Names: Two name fields say the same thing  keep product_name.\n",
    "\n",
    "- Descriptions: 55 are missing  either leave blank or use a placeholder.\n",
    "\n",
    "- Categories: Messy casing (e.g., \"Electronics\", \"electronics\")  merge and standardize.\n",
    "\n",
    "- Brands: Two brand fields, both messy and incomplete  combine, clean up, and fill missing with 'UNKNOWN'.\n",
    "\n",
    "- Numbers: Prices, weights, and ratings are stored as text  convert to numbers and handle errors.\n",
    "\n",
    "- Dimensions: Stored like \"10x5x2\"  split into length, width, and height. Handle blanks safely.\n",
    "\n",
    "- Color & Size: Many missing or empty  clean up and fill with 'Unknown' or 'N/A'.\n",
    "\n",
    "- Stock Info: Fields look fine  keep just one field like stock_quantity.\n",
    "\n",
    "- Supplier ID: Missing in many  clean up and fill missing with 'UNKNOWN'.\n",
    "\n",
    "- Dates: Some are missing or in different formats  parse to standard date/time format.\n",
    "\n",
    "- Active Status: Mixed values like \"yes\", True, 0  convert to proper True/False.\n",
    "\n",
    "- Duplicates: No exact duplicates found.\n",
    "\n",
    "- Relationships: Use product_id to link with orders. item_id might also be needed for certain datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd27cb3-8f01-471f-9d06-04c9aa55a7c5",
   "metadata": {},
   "source": [
    "# 3. orders_unstructured_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa74eeb-3dd2-413f-b46c-aa6cddc95b96",
   "metadata": {},
   "source": [
    "# Simple Summary of Order Data Issues (orders_unstructured_data.csv):\n",
    "\n",
    "- IDs: Two versions for order, customer, and product IDs  need to pick one clean version for each (use order_id, cust_id, product_id).\n",
    "\n",
    "- Dates: Two date fields  combine them, prefer the full timestamp (order_datetime) when available.\n",
    "\n",
    "- Quantity & Price: Duplicate fields (quantity vs qty, unit_price vs price)  merge each pair into one.\n",
    "\n",
    "- Total Amounts: total_amount likely reflects line totals; order_total might be full order value  treat them accordingly.\n",
    "\n",
    "- Shipping/Tax/Discount: These repeat across line items but belong to the overall order  handle carefully when building summary tables.\n",
    "\n",
    "- Status: Two status fields  merge and clean to values like DELIVERED, CANCELLED, etc.\n",
    "\n",
    "- Address: One field with full shipping address  can keep as-is or later break into parts.\n",
    "\n",
    "- Notes & Tracking: Mostly missing  clean and keep if useful.\n",
    "\n",
    "- Duplicates: None exactly the same, but cleanup is still needed.\n",
    "\n",
    "- Relationships:\n",
    "\n",
    "Cleaned order_id will group line items.\n",
    "\n",
    "Cleaned cust_id links to the Customers table.\n",
    "\n",
    "Cleaned product_id links to the Products table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f585d2-c905-4564-8b44-c9752ca7059c",
   "metadata": {},
   "source": [
    "# 4. reconciliation_challenge_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960e747-d179-4d42-90a2-216d8cb07a15",
   "metadata": {},
   "source": [
    "# Simple Summary of Reconciliation Data Issues (reconciliation_challenge_data.csv):\n",
    "\n",
    "- IDs:\n",
    "\n",
    "client_reference, transaction_ref, and item_reference need to be mapped to cleaned customer, order, and product IDs.\n",
    "\n",
    "- Customer Info:\n",
    "\n",
    "Fields like full_customer_name and contact_email should come from the main Customers table, not be stored here.\n",
    "\n",
    "- Dates:\n",
    "\n",
    "Convert transaction_date to standard format (YYYY-MM-DD).\n",
    "\n",
    "last_modified_timestamp is already in good format.\n",
    "\n",
    "- Status Fields:\n",
    "\n",
    "Values for payment_status and delivery_status look consistent  just standardize casing (UPPERCASE).\n",
    "\n",
    "- Categories:\n",
    "\n",
    "Fields like customer_segment, region, and product_line are fine but may need consistent casing.\n",
    "\n",
    "- Numbers:\n",
    "\n",
    "Most numeric fields are valid, but total_value doesnt match quantity_ordered * unit_cost  trust calculated value over given one.\n",
    "\n",
    "Use amount_paid as the actual amount spent (assumed per line item unless confirmed otherwise).\n",
    "\n",
    "- Notes:\n",
    "\n",
    "Mostly missing  clean and keep if useful. Combine if multiple lines per transaction.\n",
    "\n",
    "- Duplicates:\n",
    "\n",
    "No exact duplicates, but some transactions have multiple items.\n",
    "\n",
    "- Relationships:\n",
    "\n",
    "transaction_ref = order ID\n",
    "\n",
    "client_reference links to Customers\n",
    "\n",
    "item_reference links to Products\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f6013-511c-44e1-adb2-b1e87bcf4310",
   "metadata": {},
   "source": [
    "# Phase 2: ETL Pipeline Development\n",
    "2.1 Utility/Helper Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acbb80-b8f4-45d8-85a7-ced25c8e45d1",
   "metadata": {},
   "source": [
    "# Key Edge Cases\n",
    "- Missing or Conflicting IDs: Generated fallback canonical IDs for missing customer/product identifiers; ensured referential integrity by dropping unmappable foreign keys with proper logging.\n",
    "\n",
    "- Inconsistent Formats: Handled messy text fields (e.g., names, addresses, booleans), unparseable dates, and non-numeric price fields using defensive parsing and cleaning functions.\n",
    "\n",
    "- Duplicate & Redundant Data: Applied coalescing strategies for overlapping fields (e.g., email vs. email_address), and de-duplicated on canonical IDs with prioritization logic.\n",
    "\n",
    "- Financial Inconsistencies: Normalized line_item_total_value using quantity  unit price; retained original total_value_provided for auditing.\n",
    "\n",
    "- Empty or Invalid Records: ETL functions safely exit or skip processing when input data is empty or critical fields are missing.\n",
    "\n",
    "- Ambiguous Mappings: Implemented layered ID resolution logic (e.g., item_id vs product_id) to ensure valid lookups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced1991d-cc0d-404f-a96a-37b628de2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,506 - INFO - Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_UNKNOWN_CATEGORICAL = 'UNKNOWN'\n",
    "DEFAULT_UNKNOWN_NUMERIC_INT = 0\n",
    "DEFAULT_UNKNOWN_NUMERIC_FLOAT = 0.0\n",
    "DEFAULT_STATUS_UNKNOWN = 'UNKNOWN'\n",
    "\n",
    "# Gender Standardization\n",
    "GENDER_MAP = {\n",
    "    'M': 'MALE', 'F': 'FEMALE', 'MALE': 'MALE', 'FEMALE': 'FEMALE', \n",
    "    'OTHER': 'OTHER', '': DEFAULT_UNKNOWN_CATEGORICAL # Handle empty string\n",
    "}\n",
    "# Customer Status Standardization\n",
    "CUSTOMER_STATUS_MAP = {\n",
    "    'ACTIVE': 'ACTIVE', 'INACTIVE': 'INACTIVE', 'PENDING': 'PENDING',\n",
    "    'SUSPENDED': 'SUSPENDED', '': DEFAULT_STATUS_UNKNOWN # Handle empty string\n",
    "}\n",
    "# Payment Status Standardization (Example - expand based on all sources)\n",
    "PAYMENT_STATUS_MAP = {\n",
    "    'COMPLETED': 'COMPLETED', 'PENDING': 'PENDING', 'FAILED': 'FAILED',\n",
    "    '': DEFAULT_STATUS_UNKNOWN\n",
    "}\n",
    "# Delivery/Order Status Standardization (Example - expand based on all sources)\n",
    "ORDER_DELIVERY_STATUS_MAP = {\n",
    "    'DELIVERED': 'DELIVERED', 'PENDING': 'PENDING', 'IN_TRANSIT': 'IN_TRANSIT',\n",
    "    'PROCESSING': 'PROCESSING', 'SHIPPED': 'SHIPPED', 'CANCELLED': 'CANCELLED',\n",
    "    'RETURNED': 'RETURNED', '': DEFAULT_STATUS_UNKNOWN\n",
    "}\n",
    "# State Standardization\n",
    "STATE_ABBREVIATION_MAP = {\n",
    "    'California': 'CA', 'New York': 'NY', 'Illinois': 'IL', 'Texas': 'TX',\n",
    "    'Pennsylvania': 'PA', 'Arizona': 'AZ',\n",
    "    'CA': 'CA', 'NY': 'NY', 'IL': 'IL', 'TX': 'TX', 'PA': 'PA', 'AZ': 'AZ'\n",
    "    # Add all US states and common variations\n",
    "}\n",
    "# City Standardization (Example - this would be much larger in reality or use a geo-library)\n",
    "CITY_NORMALIZATION_MAP = {\n",
    "    'La': 'Los Angeles', 'Losangeles': 'Los Angeles',\n",
    "    'Nyc': 'New York', 'New York City': 'New York', 'New_York': 'New York',\n",
    "    'Phila': 'Philadelphia',\n",
    "    'New_york': 'New York', # From exploration\n",
    "}\n",
    "\n",
    "def clean_string(value, case=None, default_if_empty=None):\n",
    "    \"\"\"Cleans a string: strips whitespace, optionally converts case, returns default if empty.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return default_if_empty\n",
    "    value_str = str(value).strip()\n",
    "    if not value_str and default_if_empty is not None: # if after stripping it's empty\n",
    "        return default_if_empty\n",
    "    if not value_str: # if still empty and no default_if_empty\n",
    "        return None\n",
    "\n",
    "    if case == 'lower':\n",
    "        return value_str.lower()\n",
    "    elif case == 'upper':\n",
    "        return value_str.upper()\n",
    "    elif case == 'title':\n",
    "        return value_str.title()\n",
    "    return value_str\n",
    "\n",
    "def standardize_categorical(value, mapping_dict, default_value=DEFAULT_UNKNOWN_CATEGORICAL, case_transform='upper'):\n",
    "    \"\"\"Standardizes categorical values using a mapping dictionary.\"\"\"\n",
    "    cleaned_value = clean_string(value, case=case_transform)\n",
    "    if cleaned_value is None:\n",
    "        return default_value\n",
    "    return mapping_dict.get(cleaned_value, default_value)\n",
    "\n",
    "def parse_date_robustly(date_val, output_format='%Y-%m-%d', error_val=None):\n",
    "    \"\"\"Parses various date/datetime formats and returns a string in specified format or error_val.\"\"\"\n",
    "    if pd.isna(date_val) or str(date_val).strip() == '':\n",
    "        return error_val\n",
    "    try:\n",
    "        dt_obj = parser.parse(str(date_val))\n",
    "        return dt_obj.strftime(output_format)\n",
    "    except (ValueError, TypeError, parser.ParserError) as e:\n",
    "        logger.debug(f\"Date parsing failed for '{date_val}': {e}\")\n",
    "        return error_val\n",
    "\n",
    "def to_numeric_safe(value, target_type=float, default_value=None):\n",
    "    \"\"\"\n",
    "    Safely converts a value to a specified numeric type (float or int).\n",
    "    Removes common non-numeric characters like currency symbols.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return default_value\n",
    "    \n",
    "    s_value = str(value).strip()\n",
    "    if not s_value:\n",
    "        return default_value\n",
    "        \n",
    "    # Remove currency symbols, commas (for thousands), and trailing/leading non-numeric noise common in financial strings\n",
    "    s_value = re.sub(r'[^\\d\\.\\-eE]', '', s_value) # Allow for scientific notation as well\n",
    "    \n",
    "    if not s_value or s_value == '.' or s_value == '-': # Check if only a dot/hyphen remains or empty\n",
    "        return default_value\n",
    "    \n",
    "    try:\n",
    "        if target_type == int:\n",
    "            # For int, first convert to float to handle \"123.0\" cases, then to int\n",
    "            return int(float(s_value))\n",
    "        else: # float\n",
    "            return float(s_value)\n",
    "    except ValueError:\n",
    "        logger.debug(f\"Could not convert '{value}' (cleaned: '{s_value}') to {target_type.__name__}.\")\n",
    "        return default_value\n",
    "\n",
    "def standardize_boolean_strict(value, \n",
    "                               true_values={'true', 'yes', '1', 'active', 'completed', 'delivered'},\n",
    "                               false_values={'false', 'no', '0', 'inactive', 'failed', 'pending', 'cancelled', 'returned'}):\n",
    "    \"\"\"Standardizes various inputs to Python boolean True/False, or None if ambiguous.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    cleaned_value = str(value).strip().lower()\n",
    "    if not cleaned_value: # Empty string after strip\n",
    "        return None\n",
    "\n",
    "    if cleaned_value in true_values:\n",
    "        return True\n",
    "    if cleaned_value in false_values:\n",
    "        return False\n",
    "    \n",
    "    # Handle numeric representations if they weren't in true/false_values\n",
    "    try:\n",
    "        num_value = float(cleaned_value)\n",
    "        if num_value == 1.0: return True\n",
    "        if num_value == 0.0: return False\n",
    "    except ValueError:\n",
    "        pass # Not a number, and not in string lists\n",
    "\n",
    "    logger.debug(f\"Ambiguous boolean value: '{value}'. Returning None.\")\n",
    "    return None # Ambiguous\n",
    "\n",
    "def standardize_phone_strict(phone_str):\n",
    "    if pd.isna(phone_str) or str(phone_str).strip() == '':\n",
    "        return None\n",
    "    digits = re.sub(r'\\D', '', str(phone_str))\n",
    "    \n",
    "    if len(digits) == 10: # Standard US 10-digit\n",
    "        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n",
    "    elif len(digits) == 11 and digits.startswith('1'): # US 11-digit with country code\n",
    "        return f\"+1 ({digits[1:4]}) {digits[4:7]}-{digits[7:11]}\"\n",
    "    elif len(digits) > 0: # Return cleaned digits if not a standard format\n",
    "        logger.debug(f\"Non-standard phone format for '{phone_str}', returning digits: {digits}\")\n",
    "        return digits \n",
    "    return None # No digits found\n",
    "\n",
    "def standardize_postal_code(code_str):\n",
    "    if pd.isna(code_str) or str(code_str).strip() == '':\n",
    "        return None\n",
    "    cleaned_code = clean_string(str(code_str))\n",
    "    # Try to extract a 5-digit code, or 5-4 format.\n",
    "    match = re.match(r'^(\\d{5})(-\\d{4})?$', cleaned_code)\n",
    "    if match:\n",
    "        return match.group(1) # Return just the 5-digit part\n",
    "    # Handle cases where zip might be float string like \"12345.0\"\n",
    "    if '.' in cleaned_code:\n",
    "        cleaned_code = cleaned_code.split('.')[0]\n",
    "        if re.match(r'^\\d{5}$', cleaned_code):\n",
    "            return cleaned_code\n",
    "            \n",
    "    logger.debug(f\"Could not standardize postal code: '{code_str}'. Returning cleaned or None.\")\n",
    "    return cleaned_code if cleaned_code else None\n",
    "\n",
    "\n",
    "def get_current_timestamp_str():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logger.info(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b8f9f-e450-4b36-a684-ff22d6bce5e1",
   "metadata": {},
   "source": [
    "# 2.2 Database Schema Definition and Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fd00e5-9c37-4444-b5d0-d449fa5519bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,561 - INFO - Dropped existing tables (if any).\n",
      "2025-06-23 08:48:42,566 - INFO - Customers table created.\n",
      "2025-06-23 08:48:42,571 - INFO - Products table created.\n",
      "2025-06-23 08:48:42,576 - INFO - Orders table created.\n",
      "2025-06-23 08:48:42,580 - INFO - OrderItems table created.\n",
      "2025-06-23 08:48:42,601 - INFO - Indexes created.\n",
      "2025-06-23 08:48:42,602 - INFO - Database tables and indexes created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_tables(engine):\n",
    "    \"\"\"Creates database tables based on the defined ERD.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        try:\n",
    "            # Drop tables if they exist for a clean slate (idempotency for dev)\n",
    "            connection.execute(text(\"DROP TABLE IF EXISTS OrderItems;\"))\n",
    "            connection.execute(text(\"DROP TABLE IF EXISTS Orders;\"))\n",
    "            connection.execute(text(\"DROP TABLE IF EXISTS Products;\"))\n",
    "            connection.execute(text(\"DROP TABLE IF EXISTS Customers;\"))\n",
    "            logger.info(\"Dropped existing tables (if any).\")\n",
    "\n",
    "            # Customers Table\n",
    "            connection.execute(text(\"\"\"\n",
    "            CREATE TABLE Customers (\n",
    "                customer_id TEXT PRIMARY KEY, -- Canonical CUST_XXXX\n",
    "                customer_name TEXT,\n",
    "                email TEXT, -- Should be made unique after cleaning, or handle duplicates\n",
    "                phone TEXT,\n",
    "                address_street TEXT,\n",
    "                address_city TEXT,\n",
    "                address_state TEXT,\n",
    "                address_postal_code TEXT,\n",
    "                registration_date DATE,\n",
    "                status TEXT,\n",
    "                total_orders INTEGER,\n",
    "                total_spent REAL,\n",
    "                loyalty_points INTEGER,\n",
    "                preferred_payment_method TEXT,\n",
    "                birth_date DATE,\n",
    "                age INTEGER,\n",
    "                gender TEXT,\n",
    "                segment TEXT,\n",
    "                source_customer_id_int INTEGER, -- Original int ID\n",
    "                last_updated_pipeline DATETIME\n",
    "            );\n",
    "            \"\"\"))\n",
    "            # Add UNIQUE constraint on email later if desirable after handling duplicates\n",
    "            logger.info(\"Customers table created.\")\n",
    "\n",
    "            # Products Table\n",
    "            connection.execute(text(\"\"\"\n",
    "            CREATE TABLE Products (\n",
    "                product_id TEXT PRIMARY KEY, -- Canonical PROD_XXX\n",
    "                product_name TEXT,\n",
    "                description TEXT,\n",
    "                category TEXT,\n",
    "                brand TEXT,\n",
    "                manufacturer TEXT,\n",
    "                price REAL, \n",
    "                cost REAL, \n",
    "                weight_kg REAL,\n",
    "                dim_length_cm REAL,\n",
    "                dim_width_cm REAL,\n",
    "                dim_height_cm REAL,\n",
    "                color TEXT,\n",
    "                size TEXT,\n",
    "                stock_quantity INTEGER,\n",
    "                reorder_level INTEGER,\n",
    "                supplier_id TEXT,\n",
    "                is_active BOOLEAN,\n",
    "                rating REAL,\n",
    "                product_created_date DATE,\n",
    "                product_last_updated_source DATETIME,\n",
    "                source_item_id_int INTEGER, -- Original int item_id\n",
    "                last_updated_pipeline DATETIME\n",
    "            );\n",
    "            \"\"\"))\n",
    "            logger.info(\"Products table created.\")\n",
    "\n",
    "            # Orders Table\n",
    "            connection.execute(text(\"\"\"\n",
    "            CREATE TABLE Orders (\n",
    "                order_id TEXT PRIMARY KEY, -- Canonical ORD_XXXX or TXN_XXXX\n",
    "                customer_id TEXT,\n",
    "                order_date DATETIME,\n",
    "                order_status TEXT,\n",
    "                payment_method TEXT,\n",
    "                payment_status TEXT,\n",
    "                delivery_status TEXT,\n",
    "                shipping_address_full TEXT,\n",
    "                shipping_cost_total REAL, -- Changed name for clarity\n",
    "                tax_total REAL,\n",
    "                discount_total REAL,\n",
    "                order_total_value_gross REAL,\n",
    "                order_total_value_net REAL, \n",
    "                amount_paid_total REAL,\n",
    "                tracking_number TEXT,\n",
    "                notes TEXT,\n",
    "                source_order_id_int INTEGER, \n",
    "                last_updated_pipeline DATETIME,\n",
    "                FOREIGN KEY (customer_id) REFERENCES Customers(customer_id)\n",
    "            );\n",
    "            \"\"\"))\n",
    "            logger.info(\"Orders table created.\")\n",
    "\n",
    "            # OrderItems Table\n",
    "            connection.execute(text(\"\"\"\n",
    "            CREATE TABLE OrderItems (\n",
    "                order_item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                order_id TEXT,\n",
    "                product_id TEXT,\n",
    "                customer_id TEXT, -- Denormalized for potential direct queries on items per customer\n",
    "                quantity INTEGER,\n",
    "                unit_price REAL,\n",
    "                line_item_total_value REAL,\n",
    "                line_item_discount REAL,\n",
    "                line_item_tax REAL,\n",
    "                line_item_shipping_fee REAL,\n",
    "                source_file TEXT, \n",
    "                original_line_identifier TEXT, -- To trace back to source if needed\n",
    "                last_updated_pipeline DATETIME,\n",
    "                FOREIGN KEY (order_id) REFERENCES Orders(order_id),\n",
    "                FOREIGN KEY (product_id) REFERENCES Products(product_id),\n",
    "                FOREIGN KEY (customer_id) REFERENCES Customers(customer_id) \n",
    "            );\n",
    "            \"\"\"))\n",
    "            logger.info(\"OrderItems table created.\")\n",
    "\n",
    "            # Create Indexes\n",
    "            connection.execute(text(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_customers_email ON Customers(email);\")) # Email should ideally be unique\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_orders_customer_id ON Orders(customer_id);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_orders_order_date ON Orders(order_date);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_orderitems_order_id ON OrderItems(order_id);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_orderitems_product_id ON OrderItems(product_id);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_orderitems_customer_id ON OrderItems(customer_id);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_products_category ON Products(category);\"))\n",
    "            connection.execute(text(\"CREATE INDEX IF NOT EXISTS idx_products_brand ON Products(brand);\"))\n",
    "            logger.info(\"Indexes created.\")\n",
    "            \n",
    "            connection.commit()\n",
    "            logger.info(\"Database tables and indexes created successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating tables: {e}\")\n",
    "            if connection:\n",
    "                connection.rollback()\n",
    "            raise\n",
    "\n",
    "engine = create_engine(f'sqlite:///{DB_PATH}')\n",
    "create_tables(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261e4f7-de8e-43d2-9205-b0c061c74515",
   "metadata": {},
   "source": [
    "# 2.3 Customers ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc86f5ee-53e3-4b20-b9f6-ceee214c3479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,643 - INFO - Starting Customers ETL process...\n",
      "2025-06-23 08:48:42,725 - INFO - Customers after cleaning and selection: 500 rows.\n",
      "2025-06-23 08:48:42,726 - INFO - Customers after dropping duplicates on customer_id: 500 rows.\n",
      "2025-06-23 08:48:42,728 - INFO - Customers after additionally dropping duplicates on email (if present): 500 rows.\n",
      "2025-06-23 08:48:42,749 - INFO - 500 cleaned customer records loaded/appended into Customers table.\n",
      "2025-06-23 08:48:42,750 - INFO - Customers ETL process finished.\n"
     ]
    }
   ],
   "source": [
    "def etl_customers(df_raw_cust, db_engine):\n",
    "    logger.info(\"Starting Customers ETL process...\")\n",
    "    if df_raw_cust.empty:\n",
    "        logger.warning(\"Raw customer DataFrame is empty. Skipping ETL.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = df_raw_cust.copy()\n",
    "    pipeline_timestamp = get_current_timestamp_str()\n",
    "\n",
    "    # 1. ID Unification and Source ID\n",
    "    df['customer_id'] = df['cust_id'].apply(lambda x: clean_string(x, case='upper'))\n",
    "    df['source_customer_id_int'] = df['customer_id'].apply(lambda x: to_numeric_safe(x, target_type=int)) # Original int customer_id\n",
    "    \n",
    "    # Handle cases where canonical customer_id might still be missing if cust_id was bad\n",
    "    missing_cust_id_mask = df['customer_id'].isnull()\n",
    "    df.loc[missing_cust_id_mask, 'customer_id'] = 'CUST_UNKNOWN_' + df.loc[missing_cust_id_mask, 'source_customer_id_int'].astype(str)\n",
    "    df.loc[missing_cust_id_mask & df['source_customer_id_int'].isnull(), 'customer_id'] = 'CUST_UNKNOWN_INVALID'\n",
    "\n",
    "\n",
    "    # 2. Name: Coalesce and standardize\n",
    "    # Prefer customer_name if it's not username-like, else full_name if not username-like, else first non-null\n",
    "    def choose_name(row):\n",
    "        cn = str(row['customer_name'])\n",
    "        fn = str(row['full_name'])\n",
    "        is_cn_username = bool(re.match(r'^[a-z\\._\\-0-9@]+$', cn.lower()))\n",
    "        is_fn_username = bool(re.match(r'^[a-z\\._\\-0-9@]+$', fn.lower()))\n",
    "\n",
    "        if pd.notna(row['customer_name']) and not is_cn_username:\n",
    "            return clean_string(cn, 'title')\n",
    "        if pd.notna(row['full_name']) and not is_fn_username:\n",
    "            return clean_string(fn, 'title')\n",
    "        if pd.notna(row['customer_name']): # Fallback to customer_name even if username-like\n",
    "             return clean_string(cn, 'title')\n",
    "        if pd.notna(row['full_name']): # Fallback to full_name even if username-like\n",
    "             return clean_string(fn, 'title')\n",
    "        return DEFAULT_UNKNOWN_CATEGORICAL\n",
    "        \n",
    "    df['customer_name'] = df.apply(choose_name, axis=1)\n",
    "\n",
    "    # 3. Email: Coalesce, clean, handle empty.\n",
    "    df['email_temp'] = df['email'].replace('', pd.NA).apply(lambda x: clean_string(x, 'lower') if pd.notna(x) else None)\n",
    "    df['email_address_temp'] = df['email_address'].replace('', pd.NA).apply(lambda x: clean_string(x, 'lower') if pd.notna(x) else None)\n",
    "    df['email'] = df['email_temp'].fillna(df['email_address_temp'])\n",
    "    # Further validation for email format can be added here if needed.\n",
    "\n",
    "    # 4. Phone: Coalesce and standardize\n",
    "    df['phone_temp'] = df['phone'].replace('', pd.NA)\n",
    "    df['phone_number_temp'] = df['phone_number'].replace('', pd.NA)\n",
    "    df['phone'] = df['phone_number_temp'].fillna(df['phone_temp']).apply(standardize_phone_strict)\n",
    "\n",
    "    # 5. Address\n",
    "    df['address_street'] = df['address'].apply(lambda x: clean_string(x, 'title'))\n",
    "    df['address_city'] = df['city'].apply(lambda x: standardize_categorical(x, CITY_NORMALIZATION_MAP, default_value=clean_string(x, 'title'), case_transform='title'))\n",
    "    df['address_state'] = df['state'].apply(lambda x: standardize_categorical(x, STATE_ABBREVIATION_MAP, default_value=clean_string(x, 'upper'), case_transform='title'))\n",
    "    \n",
    "    df['postal_code_temp'] = df['postal_code'].replace('', pd.NA).fillna(df['zip_code'].replace('', pd.NA))\n",
    "    df['address_postal_code'] = df['postal_code_temp'].apply(standardize_postal_code)\n",
    "\n",
    "    # 6. Dates\n",
    "    df['reg_date_temp'] = df['reg_date'].replace('', pd.NA)\n",
    "    df['registration_date_temp'] = df['registration_date'].replace('', pd.NA)\n",
    "    df['registration_date'] = df['registration_date_temp'].fillna(df['reg_date_temp']).apply(parse_date_robustly)\n",
    "    df['birth_date'] = df['birth_date'].apply(parse_date_robustly)\n",
    "    \n",
    "    # 7. Status\n",
    "    df['status_temp'] = df['status'].replace('', pd.NA)\n",
    "    df['customer_status_temp'] = df['customer_status'].replace('', pd.NA)\n",
    "    df['status'] = df['customer_status_temp'].fillna(df['status_temp']).apply(lambda x: standardize_categorical(x, CUSTOMER_STATUS_MAP, default_value=DEFAULT_STATUS_UNKNOWN))\n",
    "\n",
    "\n",
    "    # 8. Numeric\n",
    "    df['total_spent'] = df['total_spent'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df['total_orders'] = df['total_orders'].apply(lambda x: to_numeric_safe(x, target_type=int, default_value=0))\n",
    "    df['loyalty_points'] = df['loyalty_points'].apply(lambda x: to_numeric_safe(x, target_type=int, default_value=0))\n",
    "\n",
    "    # 9. Age (recalculate if birth_date is available, otherwise use provided age)\n",
    "    def calculate_age(birth_date_str):\n",
    "        if pd.isna(birth_date_str): return None\n",
    "        try:\n",
    "            birth_dt = datetime.strptime(birth_date_str, '%Y-%m-%d')\n",
    "            today = datetime.today()\n",
    "            return today.year - birth_dt.year - ((today.month, today.day) < (birth_dt.month, birth_dt.day))\n",
    "        except (ValueError, TypeError): return None\n",
    "            \n",
    "    df['age_calculated'] = df['birth_date'].apply(calculate_age)\n",
    "    df['age'] = df['age_calculated'].fillna(df['age'].apply(lambda x: to_numeric_safe(x, target_type=int)))\n",
    "\n",
    "\n",
    "    # 10. Gender\n",
    "    df['gender'] = df['gender'].apply(lambda x: standardize_categorical(x, GENDER_MAP, default_value=DEFAULT_UNKNOWN_CATEGORICAL, case_transform='title')) # Title case for initial match\n",
    "\n",
    "    # 11. Segment & Payment Method\n",
    "    df['segment'] = df['segment'].apply(lambda x: clean_string(x, 'upper', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "    df['preferred_payment_method'] = df['preferred_payment'].apply(lambda x: clean_string(x, 'lower', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "\n",
    "    # Select and rename columns for the target schema\n",
    "    df_final_customers = df[[\n",
    "        'customer_id', 'customer_name', 'email', 'phone',\n",
    "        'address_street', 'address_city', 'address_state', 'address_postal_code',\n",
    "        'registration_date', 'status', 'total_orders', 'total_spent',\n",
    "        'loyalty_points', 'preferred_payment_method', 'birth_date', 'age', 'gender', \n",
    "        'segment', 'source_customer_id_int'\n",
    "    ]]\n",
    "    \n",
    "    # Handle records with missing primary key after cleaning\n",
    "    df_final_customers = df_final_customers.dropna(subset=['customer_id'])\n",
    "    logger.info(f\"Customers after cleaning and selection: {df_final_customers.shape[0]} rows.\")\n",
    "    \n",
    "    # Deduplication: Based on canonical customer_id. If emails are unique constraint, this needs more.\n",
    "    # For now, keep first. In production, might merge based on latest registration_date or other logic.\n",
    "    df_final_customers = df_final_customers.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "    logger.info(f\"Customers after dropping duplicates on customer_id: {df_final_customers.shape[0]} rows.\")\n",
    "\n",
    "    # Deduplicate based on email if it's meant to be unique and isn't None\n",
    "    df_final_customers_with_email = df_final_customers[df_final_customers['email'].notna()]\n",
    "    df_final_customers_null_email = df_final_customers[df_final_customers['email'].isna()]\n",
    "    df_final_customers_with_email = df_final_customers_with_email.drop_duplicates(subset=['email'], keep='first')\n",
    "    df_final_customers = pd.concat([df_final_customers_with_email, df_final_customers_null_email], ignore_index=True)\n",
    "    logger.info(f\"Customers after additionally dropping duplicates on email (if present): {df_final_customers.shape[0]} rows.\")\n",
    "\n",
    "    df_final_customers['last_updated_pipeline'] = pipeline_timestamp\n",
    "    \n",
    "    # Load into SQLite\n",
    "    try:\n",
    "        df_final_customers.to_sql('Customers', db_engine, if_exists='append', index=False)\n",
    "        logger.info(f\"{len(df_final_customers)} cleaned customer records loaded/appended into Customers table.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading cleaned customer data to SQLite: {e}\")\n",
    "        # Consider what to do on error: raise, log and continue, etc.\n",
    "        # For this exercise, we log and potentially skip loading this batch if fatal.\n",
    "\n",
    "    logger.info(\"Customers ETL process finished.\")\n",
    "    return df_final_customers\n",
    "\n",
    "# Run Customer ETL\n",
    "df_customers_final_loaded = etl_customers(df_customers_raw, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9998cc-187e-451a-810a-5b24135a82f3",
   "metadata": {},
   "source": [
    "# 2.4 Products ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0443fd-6cc6-4617-a0f0-09bdc06043f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,775 - INFO - Starting Products ETL process...\n",
      "2025-06-23 08:48:42,825 - INFO - Products after cleaning and selection: 200 rows.\n",
      "2025-06-23 08:48:42,826 - INFO - Products after dropping duplicates on product_id: 200 rows.\n",
      "2025-06-23 08:48:42,841 - INFO - 200 cleaned product records loaded/appended into Products table.\n",
      "2025-06-23 08:48:42,842 - INFO - Products ETL process finished.\n",
      "2025-06-23 08:48:42,857 - INFO - Product ID mapping dictionary created with 400 entries.\n",
      "2025-06-23 08:48:42,862 - INFO - Fetched 500 existing customer IDs and 200 existing product IDs from DB.\n"
     ]
    }
   ],
   "source": [
    "def etl_products(df_raw_prod, db_engine):\n",
    "    logger.info(\"Starting Products ETL process...\")\n",
    "    if df_raw_prod.empty:\n",
    "        logger.warning(\"Raw product DataFrame is empty. Skipping ETL.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = df_raw_prod.copy()\n",
    "    pipeline_timestamp = get_current_timestamp_str()\n",
    "\n",
    "    df['product_id'] = df['product_id'].apply(lambda x: clean_string(x, 'upper'))\n",
    "    df['source_item_id_int'] = df['item_id'].apply(lambda x: to_numeric_safe(x, target_type=int))\n",
    "    \n",
    "    df['product_name'] = df['product_name'].fillna(df['item_name']).apply(lambda x: clean_string(x, 'title'))\n",
    "    df['description'] = df['description'].apply(lambda x: clean_string(x) if pd.notna(x) else \"No description available\")\n",
    "\n",
    "    # Category: Coalesce and standardize. Example: prefer 'category' then 'product_category'\n",
    "    df['category_temp'] = df['category'].fillna(df['product_category']).apply(lambda x: clean_string(x, 'title'))\n",
    "    # If you have a predefined list of categories, map to that. Otherwise, just clean.\n",
    "    df['category'] = df['category_temp'].fillna(DEFAULT_UNKNOWN_CATEGORICAL)\n",
    "\n",
    "    # Brand/Manufacturer: Coalesce, clean, standardize.\n",
    "    df['brand_temp'] = df['brand'].replace('', pd.NA).fillna(df['manufacturer'].replace('', pd.NA))\n",
    "    df['brand'] = df['brand_temp'].apply(lambda x: clean_string(x, 'upper', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "    \n",
    "    df['manufacturer_temp'] = df['manufacturer'].replace('', pd.NA).fillna(df['brand'].replace('', pd.NA))\n",
    "    df['manufacturer'] = df['manufacturer_temp'].apply(lambda x: clean_string(x, 'upper', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "\n",
    "\n",
    "    df['price'] = df['price'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=DEFAULT_UNKNOWN_NUMERIC_FLOAT))\n",
    "    df['cost'] = df['cost'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=DEFAULT_UNKNOWN_NUMERIC_FLOAT))\n",
    "    df['weight_kg'] = df['weight'].apply(lambda x: to_numeric_safe(x, target_type=float)) # Null if unparseable\n",
    "    df['rating'] = df['rating'].apply(lambda x: to_numeric_safe(x, target_type=float))\n",
    "\n",
    "    def parse_dimensions_strict(dim_str):\n",
    "        if pd.isna(dim_str) or str(dim_str).strip() == '':\n",
    "            return None, None, None\n",
    "        parts = str(dim_str).lower().split('x')\n",
    "        if len(parts) == 3:\n",
    "            l = to_numeric_safe(parts[0], target_type=float)\n",
    "            w = to_numeric_safe(parts[1], target_type=float)\n",
    "            h = to_numeric_safe(parts[2], target_type=float)\n",
    "            return l, w, h\n",
    "        return None, None, None\n",
    "\n",
    "    dims = df['dimensions'].apply(parse_dimensions_strict)\n",
    "    df['dim_length_cm'] = dims.apply(lambda x: x[0] if x else None)\n",
    "    df['dim_width_cm'] = dims.apply(lambda x: x[1] if x else None)\n",
    "    df['dim_height_cm'] = dims.apply(lambda x: x[2] if x else None)\n",
    "\n",
    "    df['color'] = df['color'].replace('', pd.NA).apply(lambda x: clean_string(x, 'title', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "    df['size'] = df['size'].replace('', pd.NA).apply(lambda x: clean_string(x, 'upper', 'N/A'))\n",
    "\n",
    "    df['stock_quantity'] = df['stock_quantity'].fillna(df['stock_level']).apply(lambda x: to_numeric_safe(x, target_type=int, default_value=DEFAULT_UNKNOWN_NUMERIC_INT))\n",
    "    df['reorder_level'] = df['reorder_level'].apply(lambda x: to_numeric_safe(x, target_type=int, default_value=DEFAULT_UNKNOWN_NUMERIC_INT))\n",
    "    \n",
    "    df['supplier_id'] = df['supplier_id'].apply(lambda x: clean_string(x, 'upper', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "    \n",
    "    df['is_active'] = df['is_active'].apply(standardize_boolean_strict) # Returns True, False, or None\n",
    "\n",
    "    df['product_created_date'] = df['created_date'].apply(lambda x: parse_date_robustly(x))\n",
    "    df['product_last_updated_source'] = df['last_updated'].apply(lambda x: parse_date_robustly(x, output_format='%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    target_product_cols = [\n",
    "        'product_id', 'product_name', 'description', 'category', 'brand', 'manufacturer',\n",
    "        'price', 'cost', 'weight_kg', 'dim_length_cm', 'dim_width_cm', 'dim_height_cm',\n",
    "        'color', 'size', 'stock_quantity', 'reorder_level', 'supplier_id', 'is_active',\n",
    "        'rating', 'product_created_date', 'product_last_updated_source', 'source_item_id_int'\n",
    "    ]\n",
    "    df_final_products = df[target_product_cols]\n",
    "    \n",
    "    df_final_products = df_final_products.dropna(subset=['product_id'])\n",
    "    logger.info(f\"Products after cleaning and selection: {df_final_products.shape[0]} rows.\")\n",
    "    \n",
    "    df_final_products = df_final_products.drop_duplicates(subset=['product_id'], keep='first')\n",
    "    logger.info(f\"Products after dropping duplicates on product_id: {df_final_products.shape[0]} rows.\")\n",
    "    \n",
    "    df_final_products['last_updated_pipeline'] = pipeline_timestamp\n",
    "\n",
    "    try:\n",
    "        df_final_products.to_sql('Products', db_engine, if_exists='append', index=False)\n",
    "        logger.info(f\"{len(df_final_products)} cleaned product records loaded/appended into Products table.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading cleaned product data to SQLite: {e}\")\n",
    "        \n",
    "    logger.info(\"Products ETL process finished.\")\n",
    "    return df_final_products\n",
    "\n",
    "# Run Product ETL\n",
    "df_products_final_loaded = etl_products(df_products_raw, engine)\n",
    "\n",
    "# --- Build Product ID Mapping for Orders ---\n",
    "# This map will be crucial for OrderItems ETL\n",
    "# It maps source integer item_id (from products.json) to canonical PROD_XXX product_id\n",
    "# It also maps PROD_XXX to itself for direct lookups from orders_unstructured\n",
    "product_id_mapping_dict = {}\n",
    "if not df_products_final_loaded.empty:\n",
    "    # Map from source_item_id_int (original item_id from JSON) to canonical product_id\n",
    "    for _, row in df_products_final_loaded.iterrows():\n",
    "        if pd.notna(row['source_item_id_int']):\n",
    "            product_id_mapping_dict[str(int(row['source_item_id_int']))] = row['product_id']\n",
    "        # Also map canonical product_id to itself for direct lookups\n",
    "        if pd.notna(row['product_id']):\n",
    "            product_id_mapping_dict[str(row['product_id'])] = row['product_id']\n",
    "    logger.info(f\"Product ID mapping dictionary created with {len(product_id_mapping_dict)} entries.\")\n",
    "else:\n",
    "    logger.warning(\"Product mapping dictionary could not be built as df_products_final_loaded is empty.\")\n",
    "\n",
    "# Fetch existing canonical IDs for FK checks (now that tables are populated)\n",
    "existing_customer_ids_set = set(pd.read_sql_query(\"SELECT DISTINCT customer_id FROM Customers\", engine)['customer_id'])\n",
    "existing_product_ids_set = set(pd.read_sql_query(\"SELECT DISTINCT product_id FROM Products\", engine)['product_id'])\n",
    "logger.info(f\"Fetched {len(existing_customer_ids_set)} existing customer IDs and {len(existing_product_ids_set)} existing product IDs from DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315e9cb-37cd-478d-adc8-d2adfc9b0ba2",
   "metadata": {},
   "source": [
    "# 2.5 Orders & OrderItems ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6840ab19-8acb-4ddf-a226-851a4d1c7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,894 - INFO - Re-running ETL for reconciliation_challenge_data with corrected product AND customer ID mapping AND notes handling.\n",
      "2025-06-23 08:48:42,895 - INFO - Starting ETL for Order Items from reconciliation_challenge_data...\n",
      "2025-06-23 08:48:42,901 - INFO - Customer ID mapping in reconciliation: 300 of 300 initially attempted.\n",
      "2025-06-23 08:48:42,903 - INFO - Product ID mapping in reconciliation: 300 of 300 initially attempted.\n",
      "2025-06-23 08:48:42,906 - INFO - Shape after dropping NA in key IDs: (300, 23)\n",
      "2025-06-23 08:48:42,908 - INFO - Shape after filtering by existing_cust_ids: (300, 23)\n",
      "2025-06-23 08:48:42,910 - INFO - Shape after filtering by existing_prod_ids: (300, 23)\n",
      "2025-06-23 08:48:42,933 - WARNING - 300 reconciliation items show discrepancy between calculated total_value and provided total_value.\n",
      "2025-06-23 08:48:42,942 - INFO - Finished ETL for Order Items from reconciliation_challenge_data. Shape: (300, 17)\n",
      "2025-06-23 08:48:42,943 - INFO - Successfully processed reconciliation order items. Shape: (300, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Order Items from Reconciliation Data (Sample after fixes) ---\n",
      "    order_id customer_id product_id           order_date  quantity  unit_price  line_item_total_value  line_item_discount  line_item_shipping_fee  line_item_tax payment_status_derived delivery_status_derived line_item_notes  line_item_amount_paid_final original_line_identifier                        source_file last_updated_pipeline\n",
      "0  TXN_00119   CUST_0280   PROD_076  2023-02-12 00:00:00         2      159.48                 318.96               68.82                    7.84          16.62                 FAILED               DELIVERED            None                       664.50        TXN_00119_ITM_076  reconciliation_challenge_data.csv   2025-06-23 08:48:42\n",
      "1  TXN_00522   CUST_0360   PROD_015  2023-03-14 00:00:00         6      214.26                1285.56               96.22                   28.67          32.03                PENDING                 PENDING            None                       897.23        TXN_00522_ITM_015  reconciliation_challenge_data.csv   2025-06-23 08:48:42\n",
      "2  TXN_00733   CUST_0449   PROD_167  2023-06-24 00:00:00         7      296.42                2074.94               55.15                    5.67          62.67                 FAILED               DELIVERED            None                       252.18        TXN_00733_ITM_167  reconciliation_challenge_data.csv   2025-06-23 08:48:42\n",
      "3  TXN_00014   CUST_0071   PROD_142  2023-07-23 00:00:00         8      244.51                1956.08               61.96                   19.46          35.95                 FAILED              IN_TRANSIT            None                       255.92        TXN_00014_ITM_142  reconciliation_challenge_data.csv   2025-06-23 08:48:42\n",
      "4  TXN_00482   CUST_0305   PROD_105  2023-12-22 00:00:00         5      108.60                 543.00               98.82                   16.83          52.79              COMPLETED              IN_TRANSIT            None                       897.18        TXN_00482_ITM_105  reconciliation_challenge_data.csv   2025-06-23 08:48:42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5.1: Process reconciliation_challenge_data.csv (Order Items Source 1) (CORRECTED for 'notes')\n",
    "\n",
    "def etl_order_items_from_reconciliation(df_raw, existing_cust_ids_set, existing_prod_ids_set, prod_id_int_map):\n",
    "    logger.info(\"Starting ETL for Order Items from reconciliation_challenge_data...\")\n",
    "    if df_raw.empty:\n",
    "        logger.warning(\"Raw reconciliation_challenge_data DataFrame is empty. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = df_raw.copy()\n",
    "    pipeline_timestamp = get_current_timestamp_str()\n",
    "\n",
    "    # Rename columns to align with target OrderItems and intermediate needs\n",
    "    df.rename(columns={\n",
    "        'client_reference': 'customer_id_source', \n",
    "        'transaction_ref': 'order_id_source',\n",
    "        'item_reference': 'product_id_source_raw', \n",
    "        'transaction_date': 'order_date_source',\n",
    "        'amount_paid': 'line_item_amount_paid_source', \n",
    "        'payment_status': 'payment_status_source',\n",
    "        'delivery_status': 'delivery_status_source', \n",
    "        'quantity_ordered': 'quantity',\n",
    "        'unit_cost': 'unit_price_source', \n",
    "        'total_value': 'total_value_provided',\n",
    "        'discount_applied': 'line_item_discount_source', \n",
    "        'shipping_fee': 'line_item_shipping_fee_source',\n",
    "        'tax_amount': 'line_item_tax_source', \n",
    "        'notes_comments': 'line_item_notes_original' # Rename original to avoid conflict before cleaning\n",
    "    }, inplace=True)\n",
    "\n",
    "    df['order_id'] = df['order_id_source'].apply(lambda x: clean_string(x, 'upper'))\n",
    "    \n",
    "    def map_recon_customer_id(client_ref_val, canonical_customer_ids_set_local):\n",
    "        if pd.isna(client_ref_val): return None\n",
    "        cleaned_client_ref = clean_string(str(client_ref_val), 'upper')\n",
    "        if cleaned_client_ref and cleaned_client_ref.startswith('CLI_'):\n",
    "            potential_cust_id = cleaned_client_ref.replace('CLI_', 'CUST_')\n",
    "            if potential_cust_id in canonical_customer_ids_set_local:\n",
    "                return potential_cust_id\n",
    "        logger.debug(f\"Could not map client_reference: {client_ref_val} to a known customer_id.\")\n",
    "        return None\n",
    "\n",
    "    df['customer_id'] = df['customer_id_source'].apply(\n",
    "        lambda x: map_recon_customer_id(x, existing_cust_ids_set)\n",
    "    )\n",
    "    \n",
    "    def map_recon_product_id_corrected(item_ref_val, product_int_to_canonical_map, canonical_prod_ids_set_local):\n",
    "        if pd.isna(item_ref_val): return None\n",
    "        cleaned_item_ref = clean_string(item_ref_val, 'upper') \n",
    "        if cleaned_item_ref in canonical_prod_ids_set_local:\n",
    "            return cleaned_item_ref\n",
    "        match = re.search(r'\\d+$', cleaned_item_ref) \n",
    "        if match:\n",
    "            item_num_str = str(int(match.group(0))) \n",
    "            if item_num_str in product_int_to_canonical_map:\n",
    "                return product_int_to_canonical_map[item_num_str]\n",
    "        logger.debug(f\"Could not map product_id_source_raw from reconciliation: {item_ref_val}\")\n",
    "        return None\n",
    "            \n",
    "    df['product_id'] = df['product_id_source_raw'].apply(\n",
    "        lambda x: map_recon_product_id_corrected(x, prod_id_int_map, existing_prod_ids_set)\n",
    "    )\n",
    "\n",
    "    mapped_cust_count = df['customer_id'].notna().sum()\n",
    "    logger.info(f\"Customer ID mapping in reconciliation: {mapped_cust_count} of {len(df_raw)} initially attempted.\")\n",
    "    mapped_prod_count = df['product_id'].notna().sum()\n",
    "    logger.info(f\"Product ID mapping in reconciliation: {mapped_prod_count} of {len(df_raw)} initially attempted.\")\n",
    "    \n",
    "    # Log unmapped samples if any (for debugging)\n",
    "    if mapped_cust_count < len(df_raw):\n",
    "        unmapped_cust_sample = df[df['customer_id'].isnull()]['customer_id_source'].value_counts().head(5)\n",
    "        if not unmapped_cust_sample.empty: logger.warning(f\"Sample unmapped customer_id_source values:\\n{unmapped_cust_sample}\")\n",
    "    if mapped_prod_count < len(df_raw):\n",
    "        unmapped_prod_sample = df[df['product_id'].isnull()]['product_id_source_raw'].value_counts().head(5)\n",
    "        if not unmapped_prod_sample.empty: logger.warning(f\"Sample unmapped product_id_source_raw values:\\n{unmapped_prod_sample}\")\n",
    "\n",
    "    df.dropna(subset=['order_id', 'customer_id', 'product_id'], inplace=True)\n",
    "    logger.info(f\"Shape after dropping NA in key IDs: {df.shape}\")\n",
    "    \n",
    "    # These filters are effectively applied by dropna if IDs become None after mapping\n",
    "    df = df[df['customer_id'].isin(existing_cust_ids_set)] \n",
    "    logger.info(f\"Shape after filtering by existing_cust_ids: {df.shape}\")\n",
    "    \n",
    "    df = df[df['product_id'].isin(existing_prod_ids_set)]\n",
    "    logger.info(f\"Shape after filtering by existing_prod_ids: {df.shape}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"No valid records after ID mapping/filtering in reconciliation data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df['order_date'] = df['order_date_source'].apply(lambda x: parse_date_robustly(x, output_format='%Y-%m-%d %H:%M:%S'))\n",
    "    df['quantity'] = df['quantity'].apply(lambda x: to_numeric_safe(x, target_type=int, default_value=1))\n",
    "    df['unit_price'] = df['unit_price_source'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df['line_item_total_value'] = df['quantity'] * df['unit_price']\n",
    "    \n",
    "    df['total_value_provided_numeric'] = df['total_value_provided'].apply(to_numeric_safe)\n",
    "    discrepancy_check = ~np.isclose(df['line_item_total_value'], df['total_value_provided_numeric'].fillna(df['line_item_total_value']))\n",
    "    if discrepancy_check.any():\n",
    "        logger.warning(f\"{discrepancy_check.sum()} reconciliation items show discrepancy between calculated total_value and provided total_value.\")\n",
    "\n",
    "    df['line_item_discount'] = df['line_item_discount_source'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df['line_item_shipping_fee'] = df['line_item_shipping_fee_source'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df['line_item_tax'] = df['line_item_tax_source'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df['line_item_amount_paid_final'] = df['line_item_amount_paid_source'].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0)) \n",
    "    \n",
    "    df['payment_status_derived'] = df['payment_status_source'].apply(lambda x: standardize_categorical(x, PAYMENT_STATUS_MAP))\n",
    "    df['delivery_status_derived'] = df['delivery_status_source'].apply(lambda x: standardize_categorical(x, ORDER_DELIVERY_STATUS_MAP))\n",
    "    \n",
    "    # Clean the original notes column and assign it to 'line_item_notes'\n",
    "    if 'line_item_notes_original' in df.columns:\n",
    "         df['line_item_notes'] = df['line_item_notes_original'].apply(lambda x: clean_string(x))\n",
    "    else: # Should not happen if 'notes_comments' existed in raw\n",
    "        df['line_item_notes'] = None\n",
    "\n",
    "\n",
    "    df['original_line_identifier'] = df['order_id_source'].astype(str) + \"_\" + df['product_id_source_raw'].astype(str)\n",
    "    df['source_file'] = 'reconciliation_challenge_data.csv'\n",
    "    df['last_updated_pipeline'] = pipeline_timestamp\n",
    "\n",
    "    # Ensure all columns needed for df_final are present in df\n",
    "    final_cols_for_df1 = [\n",
    "        'order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price',\n",
    "        'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax',\n",
    "        'payment_status_derived', 'delivery_status_derived', 'line_item_notes', \n",
    "        'line_item_amount_paid_final', 'original_line_identifier', 'source_file', 'last_updated_pipeline'\n",
    "    ]\n",
    "    for col in final_cols_for_df1:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None # Add if missing, though should be there\n",
    "            logger.warning(f\"Column '{col}' was unexpectedly missing in df_final preparation for reconciliation data and was added as None.\")\n",
    "\n",
    "    df_final = df[final_cols_for_df1]\n",
    "    \n",
    "    logger.info(f\"Finished ETL for Order Items from reconciliation_challenge_data. Shape: {df_final.shape}\")\n",
    "    return df_final\n",
    "\n",
    "# --- Re-run the execution for this specific ETL step ---\n",
    "logger.info(\"Re-running ETL for reconciliation_challenge_data with corrected product AND customer ID mapping AND notes handling.\")\n",
    "if 'df_reconciliation_raw' in globals() and not df_reconciliation_raw.empty:\n",
    "    if 'existing_customer_ids_set' not in globals() or 'existing_product_ids_set' not in globals() or 'product_id_mapping_dict' not in globals():\n",
    "        logger.error(\"CRITICAL: Prerequisite ID sets or mapping dictionary for reconciliation ETL not found.\")\n",
    "        df_order_items_source1 = pd.DataFrame()\n",
    "    else:\n",
    "        df_order_items_source1 = etl_order_items_from_reconciliation(\n",
    "            df_reconciliation_raw, \n",
    "            existing_customer_ids_set, \n",
    "            existing_product_ids_set,\n",
    "            product_id_mapping_dict \n",
    "        )\n",
    "        if df_order_items_source1 is not None and not df_order_items_source1.empty:\n",
    "            logger.info(f\"Successfully processed reconciliation order items. Shape: {df_order_items_source1.shape}\")\n",
    "            print(\"\\n--- Processed Order Items from Reconciliation Data (Sample after fixes) ---\")\n",
    "            print(df_order_items_source1.head())\n",
    "        else:\n",
    "            logger.warning(\"ETL for reconciliation order items resulted in an empty or None DataFrame AFTER CORRECTIONS.\")\n",
    "else:\n",
    "    logger.warning(\"Raw reconciliation data (df_reconciliation_raw) is empty or not defined.\")\n",
    "    df_order_items_source1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0ccd8-918d-4764-9484-82ec810db342",
   "metadata": {},
   "source": [
    "# 2.5.2 Process orders_unstructured_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f4d48d-0658-4043-8042-6c2798cec927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:42,971 - INFO - Prerequisites found. Calling etl_order_items_from_unstructured.\n",
      "2025-06-23 08:48:42,974 - INFO - Starting ETL for Order Items from orders_unstructured_data...\n",
      "2025-06-23 08:48:43,008 - INFO - Shape after initial ID mapping and before filtering: (1000, 24)\n",
      "2025-06-23 08:48:43,011 - INFO - Sample of resolved IDs before filtering:\n",
      "    order_id customer_id product_id\n",
      "0  ORD_00001   CUST_0295   PROD_140\n",
      "1  ORD_00002   CUST_0087   PROD_033\n",
      "2  ORD_00003   CUST_0297   PROD_172\n",
      "3  ORD_00004   CUST_0018   PROD_006\n",
      "4  ORD_00005   CUST_0035   PROD_010\n",
      "2025-06-23 08:48:43,014 - INFO - Rows after dropping NA in key IDs: 1000. Dropped: 0\n",
      "2025-06-23 08:48:43,016 - INFO - Rows after filtering by existing_cust_ids: 1000. Dropped: 0\n",
      "2025-06-23 08:48:43,018 - INFO - Rows after filtering by existing_prod_ids: 1000. Dropped: 0\n",
      "/tmp/ipykernel_357296/2421494245.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['source_file'] = 'orders_unstructured_data.csv'\n",
      "/tmp/ipykernel_357296/2421494245.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['last_updated_pipeline'] = pipeline_timestamp\n",
      "2025-06-23 08:48:43,113 - INFO - Finished ETL for Order Items from orders_unstructured_data. Shape: (1000, 20)\n",
      "2025-06-23 08:48:43,114 - INFO - Successfully processed unstructured order items. Shape: (1000, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Order Items from Unstructured Data (Sample) ---\n",
      "    order_id customer_id product_id           order_date  quantity  unit_price  line_item_total_value  line_item_discount  line_item_shipping_fee  line_item_tax line_item_status payment_method shipping_address_full                          notes tracking_number  source_order_id_int  line_item_amount_paid original_line_identifier                   source_file last_updated_pipeline\n",
      "0  ORD_00001   CUST_0295   PROD_140  2023-06-01 00:39:00         6      381.29                 774.00                9.27                    6.90          50.40          SHIPPED     debit_card          4136 Main St  Special delivery instructions            None                    1                 822.03   ORD_00001_PROD_140_116  orders_unstructured_data.csv   2025-06-23 08:48:42\n",
      "1  ORD_00002   CUST_0087   PROD_033  2023-11-22 17:08:00         8      153.77                 152.92               36.07                   16.45          27.77        DELIVERED  bank_transfer          3133 Oak Ave                           None            None                    2                 161.07    ORD_00002_PROD_033_38  orders_unstructured_data.csv   2025-06-23 08:48:42\n",
      "2  ORD_00003   CUST_0297   PROD_172  2023-01-11 15:58:00         1       94.72                 583.22                0.00                    9.89           3.94        CANCELLED         paypal          7377 Main St                           None            None                    3                 597.05    ORD_00003_PROD_172_27  orders_unstructured_data.csv   2025-06-23 08:48:42\n",
      "3  ORD_00004   CUST_0018   PROD_006  2023-09-22 00:17:00         1       76.03                 339.32                0.00                    6.92           4.51          SHIPPED     debit_card          9595 Main St                           None            None                    4                 350.75    ORD_00004_PROD_006_17  orders_unstructured_data.csv   2025-06-23 08:48:42\n",
      "4  ORD_00005   CUST_0035   PROD_010  2023-07-23 14:03:00         4      287.33                1036.75                0.00                    0.00          18.43          PENDING         paypal          9417 Oak Ave                           None       TRK258913                    5                1055.18   ORD_00005_PROD_010_138  orders_unstructured_data.csv   2025-06-23 08:48:42\n"
     ]
    }
   ],
   "source": [
    "def etl_order_items_from_unstructured(df_raw, existing_cust_ids, existing_prod_ids, prod_id_map):\n",
    "    logger.info(\"Starting ETL for Order Items from orders_unstructured_data...\")\n",
    "    if df_raw.empty:\n",
    "        logger.warning(\"Raw orders_unstructured_data DataFrame is empty. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df_working = df_raw.copy() \n",
    "    pipeline_timestamp = get_current_timestamp_str()\n",
    "\n",
    "    # --- ID Coalescing & Standardization ---\n",
    "    df_working['order_id'] = df_raw['order_id'].fillna(df_raw['ord_id'].astype(str)).apply(lambda x: clean_string(x, 'upper'))\n",
    "    \n",
    "    df_working['source_order_id_int'] = df_raw['ord_id'].fillna(\n",
    "        df_raw['order_id'].apply(lambda x: to_numeric_safe(re.sub(r'\\D', '', str(x)), target_type=int) if pd.notna(x) else None)\n",
    "    ).apply(lambda x: to_numeric_safe(x, target_type=int))\n",
    "    \n",
    "    customer_id_str_source = df_raw['cust_id'].astype(str).apply(lambda x: clean_string(x, 'upper'))\n",
    "    customer_id_int_source = df_raw['customer_id'].apply(lambda x: to_numeric_safe(x, target_type=int))\n",
    "    df_working['customer_id'] = customer_id_str_source.fillna(\n",
    "        customer_id_int_source.apply(lambda x: f\"CUST_{str(x).zfill(4)}\" if pd.notna(x) else None)\n",
    "    )\n",
    "\n",
    "    def resolve_unstructured_product_id(row_from_raw_data, product_id_lookup_map, canonical_product_id_set):\n",
    "        prod_id_val = clean_string(row_from_raw_data['product_id'], 'upper')\n",
    "        item_id_val_str = str(int(row_from_raw_data['item_id'])) if pd.notna(row_from_raw_data['item_id']) else None\n",
    "\n",
    "        if prod_id_val and prod_id_val in canonical_product_id_set:\n",
    "            return prod_id_val\n",
    "        if item_id_val_str and item_id_val_str in product_id_lookup_map:\n",
    "            return product_id_lookup_map[item_id_val_str]\n",
    "        \n",
    "        # Fallback: if item_id (string version) matches an existing PROD_XXX id\n",
    "        if item_id_val_str and item_id_val_str in canonical_product_id_set: \n",
    "             return item_id_val_str                                        \n",
    "                                                                             \n",
    "        \n",
    "        logger.debug(f\"Could not map product_id/item_id: {row_from_raw_data['product_id']}/{row_from_raw_data['item_id']}\")\n",
    "        return None\n",
    "        \n",
    "    df_working['product_id'] = df_raw.apply(\n",
    "        lambda row: resolve_unstructured_product_id(row, prod_id_map, existing_prod_ids), axis=1\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Shape after initial ID mapping and before filtering: {df_working.shape}\")\n",
    "    logger.info(f\"Sample of resolved IDs before filtering:\\n{df_working[['order_id', 'customer_id', 'product_id']].head()}\")\n",
    "\n",
    "    # --- Initial Filtering based on resolved IDs ---\n",
    "    original_count_before_dropna = len(df_working)\n",
    "    df_working.dropna(subset=['order_id', 'customer_id', 'product_id'], inplace=True)\n",
    "    logger.info(f\"Rows after dropping NA in key IDs: {len(df_working)}. Dropped: {original_count_before_dropna - len(df_working)}\")\n",
    "    \n",
    "    original_count_before_cust_filter = len(df_working)\n",
    "    df_working = df_working[df_working['customer_id'].isin(existing_cust_ids)]\n",
    "    logger.info(f\"Rows after filtering by existing_cust_ids: {len(df_working)}. Dropped: {original_count_before_cust_filter - len(df_working)}\")\n",
    "\n",
    "    original_count_before_prod_filter = len(df_working)\n",
    "    df_working = df_working[df_working['product_id'].isin(existing_prod_ids)]\n",
    "    logger.info(f\"Rows after filtering by existing_prod_ids: {len(df_working)}. Dropped: {original_count_before_prod_filter - len(df_working)}\")\n",
    "\n",
    "\n",
    "    if df_working.empty:\n",
    "        logger.warning(\"No valid records after ID mapping/filtering in unstructured orders data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- Apply transformations using .loc[df_working.index] on df_raw to ensure correct alignment ---\n",
    "    df_working['order_date'] = df_raw['order_datetime'].fillna(df_raw['order_date']).loc[df_working.index].apply(\n",
    "        lambda x: parse_date_robustly(x, output_format='%Y-%m-%d %H:%M:%S')\n",
    "    )\n",
    "\n",
    "    df_working['quantity'] = df_raw['quantity'].fillna(df_raw['qty']).loc[df_working.index].apply(\n",
    "        lambda x: to_numeric_safe(x, target_type=int, default_value=1)\n",
    "    )\n",
    "    df_working['unit_price'] = df_raw['unit_price'].fillna(df_raw['price']).loc[df_working.index].apply(\n",
    "        lambda x: to_numeric_safe(x, target_type=float, default_value=0.0)\n",
    "    )\n",
    "    \n",
    "    df_working['calculated_line_total'] = df_working['quantity'] * df_working['unit_price']\n",
    "    df_working['line_item_total_value'] = df_raw['total_amount'].loc[df_working.index].apply(\n",
    "        lambda x: to_numeric_safe(x, target_type=float)\n",
    "    ).fillna(df_working['calculated_line_total'])\n",
    "    \n",
    "    df_working['line_item_discount'] = df_raw['discount'].loc[df_working.index].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df_working['line_item_tax'] = df_raw['tax'].loc[df_working.index].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    df_working['line_item_shipping_fee'] = df_raw['shipping_cost'].loc[df_working.index].apply(lambda x: to_numeric_safe(x, target_type=float, default_value=0.0))\n",
    "    \n",
    "    df_working['line_item_amount_paid'] = (df_working['line_item_total_value'] - \n",
    "                                          df_working['line_item_discount'] + \n",
    "                                          df_working['line_item_tax'] + \n",
    "                                          df_working['line_item_shipping_fee'])\n",
    "\n",
    "    status_temp = df_raw['status'].replace('',pd.NA).loc[df_working.index]\n",
    "    order_status_temp = df_raw['order_status'].replace('',pd.NA).loc[df_working.index]\n",
    "    df_working['line_item_status_source'] = order_status_temp.fillna(status_temp)\n",
    "    df_working['line_item_status'] = df_working['line_item_status_source'].apply(\n",
    "        lambda x: standardize_categorical(x, ORDER_DELIVERY_STATUS_MAP, default_value=DEFAULT_STATUS_UNKNOWN)\n",
    "    )\n",
    "\n",
    "    df_working['payment_method'] = df_raw['payment_method'].loc[df_working.index].apply(lambda x: clean_string(x, 'lower', DEFAULT_UNKNOWN_CATEGORICAL))\n",
    "    df_working['shipping_address_full'] = df_raw['shipping_address'].loc[df_working.index].apply(clean_string)\n",
    "    df_working['notes'] = df_raw['notes'].loc[df_working.index].apply(clean_string)\n",
    "    df_working['tracking_number'] = df_raw['tracking_number'].loc[df_working.index].apply(clean_string)\n",
    "    \n",
    "    df_working['original_line_identifier'] = df_working['order_id'].astype(str) + \"_\" + \\\n",
    "                                             df_working['product_id'].astype(str) + \"_\" + \\\n",
    "                                             df_raw['item_id'].loc[df_working.index].astype(str)\n",
    "\n",
    "    df_final = df_working[[\n",
    "        'order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price',\n",
    "        'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax',\n",
    "        'line_item_status', 'payment_method', 'shipping_address_full', 'notes', 'tracking_number',\n",
    "        'source_order_id_int', 'line_item_amount_paid', 'original_line_identifier'\n",
    "    ]]\n",
    "    \n",
    "    df_final['source_file'] = 'orders_unstructured_data.csv'\n",
    "    df_final['last_updated_pipeline'] = pipeline_timestamp\n",
    "    \n",
    "    logger.info(f\"Finished ETL for Order Items from orders_unstructured_data. Shape: {df_final.shape}\")\n",
    "    return df_final\n",
    "\n",
    "# --- Actual Execution Block for this ETL step ---\n",
    "# Ensure all prerequisite DataFrames and variables are defined and populated from previous cells\n",
    "if 'df_orders_unstructured_raw' in globals() and not df_orders_unstructured_raw.empty:\n",
    "    if 'existing_customer_ids_set' not in globals() or 'existing_product_ids_set' not in globals() or 'product_id_mapping_dict' not in globals():\n",
    "        logger.error(\"Prerequisite ID sets or mapping dictionary not found. Ensure Customer and Product ETL ran successfully.\")\n",
    "        df_order_items_source2 = pd.DataFrame() # Initialize as empty\n",
    "    else:\n",
    "        logger.info(\"Prerequisites found. Calling etl_order_items_from_unstructured.\")\n",
    "        df_order_items_source2 = etl_order_items_from_unstructured(\n",
    "            df_orders_unstructured_raw, \n",
    "            existing_customer_ids_set, \n",
    "            existing_product_ids_set,\n",
    "            product_id_mapping_dict \n",
    "        )\n",
    "        \n",
    "        if df_order_items_source2 is not None and not df_order_items_source2.empty:\n",
    "            logger.info(f\"Successfully processed unstructured order items. Shape: {df_order_items_source2.shape}\")\n",
    "            print(\"\\n--- Processed Order Items from Unstructured Data (Sample) ---\")\n",
    "            print(df_order_items_source2.head())\n",
    "            # print(df_order_items_source2.info()) # Use .info() for more detail if needed during debug\n",
    "        else:\n",
    "            logger.warning(\"ETL for unstructured order items resulted in an empty or None DataFrame.\")\n",
    "else:\n",
    "    logger.warning(\"Raw orders unstructured data (df_orders_unstructured_raw) is empty or not defined. Skipping ETL for this source.\")\n",
    "    df_order_items_source2 = pd.DataFrame() # Ensure it's defined as empty if not processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe604d1d-c4b7-437d-b6e6-3cfdfaf12f7c",
   "metadata": {},
   "source": [
    "# 2.5.3 Combine Order Item Data and Create Final OrderItems & Orders Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6275450b-d9eb-4e96-884b-ccb900669add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:43,160 - INFO - Preparing to call etl_combine_orders_and_load...\n",
      "2025-06-23 08:48:43,161 - INFO - df_order_items_source1 to be combined has shape: (300, 17) and columns: ['order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price', 'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax', 'payment_status_derived', 'delivery_status_derived', 'line_item_notes', 'line_item_amount_paid_final', 'original_line_identifier', 'source_file', 'last_updated_pipeline']\n",
      "2025-06-23 08:48:43,162 - INFO - df_order_items_source2 to be combined has shape: (1000, 20) and columns: ['order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price', 'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax', 'line_item_status', 'payment_method', 'shipping_address_full', 'notes', 'tracking_number', 'source_order_id_int', 'line_item_amount_paid', 'original_line_identifier', 'source_file', 'last_updated_pipeline']\n",
      "2025-06-23 08:48:43,163 - INFO - Starting to combine and finalize order items and derive orders...\n",
      "2025-06-23 08:48:43,166 - INFO - df1_processed (from reconciliation) shape: (300, 17). Has 'line_item_amount_paid_final': True\n",
      "2025-06-23 08:48:43,170 - INFO - Sample of line_item_amount_paid_final from df1_processed:\n",
      "    order_id  line_item_amount_paid_final\n",
      "0  TXN_00119                       664.50\n",
      "1  TXN_00522                       897.23\n",
      "2  TXN_00733                       252.18\n",
      "3  TXN_00014                       255.92\n",
      "4  TXN_00482                       897.18\n",
      "2025-06-23 08:48:43,173 - INFO - df2_processed (from unstructured) shape: (1000, 20). Has 'line_item_amount_paid_final': True\n",
      "2025-06-23 08:48:43,178 - INFO - Sample of line_item_amount_paid_final from df2_processed:\n",
      "    order_id  line_item_amount_paid_final\n",
      "0  ORD_00001                       822.03\n",
      "1  ORD_00002                       161.07\n",
      "2  ORD_00003                       597.05\n",
      "3  ORD_00004                       350.75\n",
      "4  ORD_00005                      1055.18\n",
      "2025-06-23 08:48:43,189 - INFO - Combined order items. Shape: (1300, 23)\n",
      "2025-06-23 08:48:43,190 - INFO - Columns in df_all_order_items after concat: ['order_item_id', 'order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price', 'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax', 'payment_status_derived', 'delivery_status_derived', 'overall_item_status_derived', 'line_item_notes', 'line_item_amount_paid_final', 'payment_method_source', 'shipping_address_full_source', 'tracking_number_source', 'source_order_id_int_val', 'source_file', 'original_line_identifier', 'last_updated_pipeline']\n",
      "2025-06-23 08:48:43,194 - INFO - Describe line_item_amount_paid_final in df_all_order_items:\n",
      "count    1300.000000\n",
      "mean      558.437115\n",
      "std       294.552263\n",
      "min        20.840000\n",
      "25%       305.790000\n",
      "50%       547.540000\n",
      "75%       810.040000\n",
      "max      1102.100000\n",
      "Name: line_item_amount_paid_final, dtype: float64\n",
      "2025-06-23 08:48:43,198 - INFO - Sample of line_item_amount_paid_final in df_all_order_items where source is unstructured:\n",
      "      order_id  line_item_amount_paid_final\n",
      "300  ORD_00001                       822.03\n",
      "301  ORD_00002                       161.07\n",
      "302  ORD_00003                       597.05\n",
      "303  ORD_00004                       350.75\n",
      "304  ORD_00005                      1055.18\n",
      "305  ORD_00006                       799.07\n",
      "306  ORD_00007                       149.21\n",
      "307  ORD_00008                       524.30\n",
      "308  ORD_00009                       883.21\n",
      "309  ORD_00010                       554.10\n",
      "2025-06-23 08:48:43,202 - INFO - Sample of line_item_amount_paid_final in df_all_order_items where source is reconciliation:\n",
      "    order_id  line_item_amount_paid_final\n",
      "0  TXN_00119                       664.50\n",
      "1  TXN_00522                       897.23\n",
      "2  TXN_00733                       252.18\n",
      "3  TXN_00014                       255.92\n",
      "4  TXN_00482                       897.18\n",
      "5  TXN_00203                       991.52\n",
      "6  TXN_00550                       694.95\n",
      "7  TXN_00572                        82.80\n",
      "8  TXN_00357                       581.73\n",
      "9  TXN_00095                       150.75\n",
      "2025-06-23 08:48:44,102 - INFO - Derived Orders table. Shape: (1265, 18)\n",
      "2025-06-23 08:48:44,104 - INFO - Sample of derived Orders with amount_paid_total:\n",
      "    order_id  amount_paid_total order_status\n",
      "0  ORD_00001             822.03      SHIPPED\n",
      "1  ORD_00002             161.07    DELIVERED\n",
      "2  ORD_00003             597.05    CANCELLED\n",
      "3  ORD_00004             350.75      SHIPPED\n",
      "4  ORD_00005            1055.18      PENDING\n",
      "2025-06-23 08:48:44,106 - INFO - Final OrderItems for DB after filtering by valid orders. Shape: (1300, 13)\n",
      "2025-06-23 08:48:44,111 - INFO - Cleared existing Orders and OrderItems data from DB for fresh load.\n",
      "2025-06-23 08:48:44,147 - INFO - 1265 records loaded/appended into Orders table.\n",
      "2025-06-23 08:48:44,175 - INFO - 1300 records loaded/appended into OrderItems table.\n",
      "2025-06-23 08:48:44,176 - INFO - Orders table loaded successfully. Final shape: (1265, 18)\n",
      "2025-06-23 08:48:44,182 - INFO - OrderItems table loaded successfully. Final shape: (1300, 13)\n",
      "2025-06-23 08:48:44,186 - INFO - --- ETL Pipeline (Combine Orders & OrderItems) Complete ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Orders Data (Sample from DB load) ---\n",
      "    order_id customer_id           order_date order_status payment_method payment_status delivery_status shipping_address_full  shipping_cost_total  tax_total  discount_total  order_total_value_gross  amount_paid_total tracking_number                          notes source_order_id_int  order_total_value_net last_updated_pipeline\n",
      "0  ORD_00001   CUST_0295  2023-06-01 00:39:00      SHIPPED     debit_card        UNKNOWN         UNKNOWN          4136 Main St                 6.90      50.40            9.27                   774.00             822.03            None  Special delivery instructions                   1                 764.73   2025-06-23 08:48:43\n",
      "1  ORD_00002   CUST_0087  2023-11-22 17:08:00    DELIVERED  bank_transfer        UNKNOWN         UNKNOWN          3133 Oak Ave                16.45      27.77           36.07                   152.92             161.07            None                           None                   2                 116.85   2025-06-23 08:48:43\n",
      "2  ORD_00003   CUST_0297  2023-01-11 15:58:00    CANCELLED         paypal        UNKNOWN         UNKNOWN          7377 Main St                 9.89       3.94            0.00                   583.22             597.05            None                           None                   3                 583.22   2025-06-23 08:48:43\n",
      "3  ORD_00004   CUST_0018  2023-09-22 00:17:00      SHIPPED     debit_card        UNKNOWN         UNKNOWN          9595 Main St                 6.92       4.51            0.00                   339.32             350.75            None                           None                   4                 339.32   2025-06-23 08:48:43\n",
      "4  ORD_00005   CUST_0035  2023-07-23 14:03:00      PENDING         paypal        UNKNOWN         UNKNOWN          9417 Oak Ave                 0.00      18.43            0.00                  1036.75            1055.18       TRK258913                           None                   5                1036.75   2025-06-23 08:48:43\n",
      "\n",
      "--- Final OrderItems Data (Sample from DB load) ---\n",
      "   order_item_id   order_id product_id customer_id  quantity  unit_price  line_item_total_value  line_item_discount  line_item_tax  line_item_shipping_fee                        source_file original_line_identifier last_updated_pipeline\n",
      "0              1  TXN_00119   PROD_076   CUST_0280         2      159.48                 318.96               68.82          16.62                    7.84  reconciliation_challenge_data.csv        TXN_00119_ITM_076   2025-06-23 08:48:42\n",
      "1              2  TXN_00522   PROD_015   CUST_0360         6      214.26                1285.56               96.22          32.03                   28.67  reconciliation_challenge_data.csv        TXN_00522_ITM_015   2025-06-23 08:48:42\n",
      "2              3  TXN_00733   PROD_167   CUST_0449         7      296.42                2074.94               55.15          62.67                    5.67  reconciliation_challenge_data.csv        TXN_00733_ITM_167   2025-06-23 08:48:42\n",
      "3              4  TXN_00014   PROD_142   CUST_0071         8      244.51                1956.08               61.96          35.95                   19.46  reconciliation_challenge_data.csv        TXN_00014_ITM_142   2025-06-23 08:48:42\n",
      "4              5  TXN_00482   PROD_105   CUST_0305         5      108.60                 543.00               98.82          52.79                   16.83  reconciliation_challenge_data.csv        TXN_00482_ITM_105   2025-06-23 08:48:42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5.3: Combine Order Item Data and Create Final OrderItems & Orders Tables (Focus on amount_paid_total)\n",
    "\n",
    "def etl_combine_orders_and_load(df_items1_input, df_items2_input, db_engine, cust_ids_set, prod_ids_set):\n",
    "    logger.info(\"Starting to combine and finalize order items and derive orders...\")\n",
    "    pipeline_timestamp = get_current_timestamp_str()\n",
    "\n",
    "    # --- Prepare df_items1 (from reconciliation_challenge_data) ---\n",
    "    df1_processed = pd.DataFrame()\n",
    "    if df_items1_input is not None and not df_items1_input.empty:\n",
    "        df1 = df_items1_input.copy()\n",
    "        # These renames should align with what etl_order_items_from_reconciliation produces\n",
    "        df1.rename(columns={\n",
    "            'payment_status': 'payment_status_derived', \n",
    "            'delivery_status': 'delivery_status_derived',\n",
    "            'notes': 'line_item_notes',\n",
    "            # 'line_item_amount_paid' from its source function is already 'line_item_amount_paid_final' effectively\n",
    "        }, inplace=True, errors='ignore') \n",
    "        \n",
    "        # Ensure 'line_item_amount_paid_final' exists (it should from the source function)\n",
    "        if 'line_item_amount_paid_final' not in df1.columns:\n",
    "            logger.warning(\"Col 'line_item_amount_paid_final' missing in df1 from reconciliation! Adding as 0.0\")\n",
    "            df1['line_item_amount_paid_final'] = 0.0\n",
    "        df1_processed = df1\n",
    "        logger.info(f\"df1_processed (from reconciliation) shape: {df1_processed.shape}. Has 'line_item_amount_paid_final': {'line_item_amount_paid_final' in df1_processed.columns}\")\n",
    "        if 'line_item_amount_paid_final' in df1_processed.columns:\n",
    "            logger.info(f\"Sample of line_item_amount_paid_final from df1_processed:\\n{df1_processed[['order_id', 'line_item_amount_paid_final']].head()}\")\n",
    "\n",
    "    else:\n",
    "        logger.info(\"df_items1_input is empty or None.\")\n",
    "\n",
    "    # --- Prepare df_items2 (from orders_unstructured_data) ---\n",
    "    df2_processed = pd.DataFrame()\n",
    "    if df_items2_input is not None and not df_items2_input.empty:\n",
    "        df2 = df_items2_input.copy()\n",
    "        # These renames should align with what etl_order_items_from_unstructured produces\n",
    "        df2.rename(columns={\n",
    "            'line_item_status': 'overall_item_status_derived',\n",
    "            'payment_method': 'payment_method_source',\n",
    "            'shipping_address_full': 'shipping_address_full_source',\n",
    "            'notes': 'line_item_notes',\n",
    "            'tracking_number': 'tracking_number_source',\n",
    "            'source_order_id_int': 'source_order_id_int_val',\n",
    "            'line_item_amount_paid': 'line_item_amount_paid_final' # This rename is key\n",
    "        }, inplace=True, errors='ignore')\n",
    "        \n",
    "        # Ensure 'line_item_amount_paid_final' exists after rename\n",
    "        if 'line_item_amount_paid_final' not in df2.columns:\n",
    "            logger.warning(\"Col 'line_item_amount_paid_final' missing in df2 from unstructured after rename! Adding as 0.0\")\n",
    "            df2['line_item_amount_paid_final'] = 0.0 # Should not happen if 'line_item_amount_paid' was in df_items2_input\n",
    "        df2_processed = df2\n",
    "        logger.info(f\"df2_processed (from unstructured) shape: {df2_processed.shape}. Has 'line_item_amount_paid_final': {'line_item_amount_paid_final' in df2_processed.columns}\")\n",
    "        if 'line_item_amount_paid_final' in df2_processed.columns:\n",
    "             logger.info(f\"Sample of line_item_amount_paid_final from df2_processed:\\n{df2_processed[['order_id', 'line_item_amount_paid_final']].head()}\")\n",
    "    else:\n",
    "        logger.info(\"df_items2_input is empty or None.\")\n",
    "\n",
    "    # --- Concatenate ---\n",
    "    if df1_processed.empty and df2_processed.empty:\n",
    "        logger.warning(\"Both processed order item DataFrames are empty. Cannot proceed.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Define a superset of columns you expect in df_all_order_items\n",
    "    # This helps ensure alignment before concat.\n",
    "    # These are columns that might exist in one but not the other, or common ones.\n",
    "    superset_cols = [\n",
    "        'order_id', 'customer_id', 'product_id', 'order_date', 'quantity', 'unit_price',\n",
    "        'line_item_total_value', 'line_item_discount', 'line_item_shipping_fee', 'line_item_tax',\n",
    "        'payment_status_derived', 'delivery_status_derived', 'overall_item_status_derived', \n",
    "        'line_item_notes', 'line_item_amount_paid_final', 'payment_method_source', \n",
    "        'shipping_address_full_source', 'tracking_number_source', 'source_order_id_int_val',\n",
    "        'source_file', 'original_line_identifier', 'last_updated_pipeline'\n",
    "    ]\n",
    "\n",
    "    df1_aligned = pd.DataFrame()\n",
    "    if not df1_processed.empty:\n",
    "        for col in superset_cols: # Ensure all columns from superset are present\n",
    "            if col not in df1_processed.columns:\n",
    "                df1_processed[col] = None\n",
    "        df1_aligned = df1_processed[superset_cols] # Select in defined order\n",
    "        \n",
    "    df2_aligned = pd.DataFrame()\n",
    "    if not df2_processed.empty:\n",
    "        for col in superset_cols: # Ensure all columns from superset are present\n",
    "            if col not in df2_processed.columns:\n",
    "                df2_processed[col] = None\n",
    "        df2_aligned = df2_processed[superset_cols]\n",
    "\n",
    "    df_all_order_items = pd.concat([df1_aligned, df2_aligned], ignore_index=True)\n",
    "    df_all_order_items.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if 'order_item_id' not in df_all_order_items.columns:\n",
    "         df_all_order_items.insert(0, 'order_item_id', range(1, 1 + len(df_all_order_items)))\n",
    "    elif df_all_order_items['order_item_id'].isnull().any():\n",
    "        df_all_order_items['order_item_id'] = range(1, 1 + len(df_all_order_items))\n",
    "\n",
    "    logger.info(f\"Combined order items. Shape: {df_all_order_items.shape}\")\n",
    "    logger.info(f\"Columns in df_all_order_items after concat: {df_all_order_items.columns.tolist()}\")\n",
    "    if 'line_item_amount_paid_final' in df_all_order_items.columns:\n",
    "        logger.info(f\"Describe line_item_amount_paid_final in df_all_order_items:\\n{df_all_order_items['line_item_amount_paid_final'].describe()}\")\n",
    "        logger.info(f\"Sample of line_item_amount_paid_final in df_all_order_items where source is unstructured:\\n{df_all_order_items[df_all_order_items['source_file'] == 'orders_unstructured_data.csv'][['order_id', 'line_item_amount_paid_final']].head(10)}\")\n",
    "        logger.info(f\"Sample of line_item_amount_paid_final in df_all_order_items where source is reconciliation:\\n{df_all_order_items[df_all_order_items['source_file'] == 'reconciliation_challenge_data.csv'][['order_id', 'line_item_amount_paid_final']].head(10)}\")\n",
    "    else:\n",
    "        logger.error(\"'line_item_amount_paid_final' column is MISSING in df_all_order_items!\")\n",
    "\n",
    "\n",
    "    # --- Derive Orders Table ---\n",
    "    if df_all_order_items.empty:\n",
    "        logger.warning(\"df_all_order_items is unexpectedly empty before deriving Orders table.\")\n",
    "        return df_all_order_items, pd.DataFrame()\n",
    "\n",
    "    df_all_order_items['order_level_status_candidate'] = \\\n",
    "        df_all_order_items['delivery_status_derived'].fillna(\n",
    "        df_all_order_items['overall_item_status_derived']).fillna(\n",
    "        df_all_order_items['payment_status_derived'] \n",
    "    ).fillna(DEFAULT_STATUS_UNKNOWN)\n",
    "    \n",
    "    aggregation_defaults = { # Ensure these match column names in df_all_order_items\n",
    "        'customer_id': None, 'order_date': None, 'order_level_status_candidate': DEFAULT_STATUS_UNKNOWN,\n",
    "        'payment_method_source': DEFAULT_UNKNOWN_CATEGORICAL, \n",
    "        'payment_status_derived': DEFAULT_STATUS_UNKNOWN,\n",
    "        'delivery_status_derived': DEFAULT_STATUS_UNKNOWN,\n",
    "        'shipping_address_full_source': None,\n",
    "        'line_item_shipping_fee': 0.0, 'line_item_tax': 0.0, 'line_item_discount': 0.0,\n",
    "        'line_item_total_value': 0.0, 'line_item_amount_paid_final': 0.0,\n",
    "        'tracking_number_source': None, 'line_item_notes': None, 'source_order_id_int_val': None\n",
    "    }\n",
    "    for col, default_val in aggregation_defaults.items(): # Check existence before groupby\n",
    "        if col not in df_all_order_items.columns:\n",
    "            df_all_order_items[col] = default_val\n",
    "            logger.info(f\"Added missing aggregation source column '{col}' to df_all_order_items for groupby.\")\n",
    "\n",
    "    df_orders = df_all_order_items.groupby('order_id', as_index=False).agg(\n",
    "        customer_id=('customer_id', 'first'),\n",
    "        order_date=('order_date', 'min'),\n",
    "        order_status=('order_level_status_candidate', lambda x: x.mode()[0] if not x.mode().empty else DEFAULT_STATUS_UNKNOWN),\n",
    "        payment_method=('payment_method_source', lambda x: x.dropna().iloc[0] if not x.dropna().empty else DEFAULT_UNKNOWN_CATEGORICAL),\n",
    "        payment_status=('payment_status_derived', lambda x: x.dropna().iloc[0] if not x.dropna().empty else DEFAULT_STATUS_UNKNOWN),\n",
    "        delivery_status=('delivery_status_derived', lambda x: x.dropna().iloc[0] if not x.dropna().empty else DEFAULT_STATUS_UNKNOWN),\n",
    "        shipping_address_full=('shipping_address_full_source', 'first'),\n",
    "        shipping_cost_total=('line_item_shipping_fee', 'sum'),\n",
    "        tax_total=('line_item_tax', 'sum'),\n",
    "        discount_total=('line_item_discount', 'sum'),\n",
    "        order_total_value_gross=('line_item_total_value', 'sum'),\n",
    "        amount_paid_total=('line_item_amount_paid_final', 'sum'), # This should now sum correctly\n",
    "        tracking_number=('tracking_number_source', 'first'),\n",
    "        notes=('line_item_notes', lambda x: '; '.join(sorted(list(x.dropna().astype(str).unique()))) if not x.dropna().empty and x.dropna().astype(str).str.len().sum() > 0 else None),\n",
    "        source_order_id_int=('source_order_id_int_val', 'first')\n",
    "    )\n",
    "    \n",
    "    df_orders['order_total_value_net'] = df_orders['order_total_value_gross'] - df_orders['discount_total']\n",
    "    df_orders['last_updated_pipeline'] = pipeline_timestamp\n",
    "    \n",
    "    df_orders = df_orders[df_orders['customer_id'].isin(cust_ids_set)]\n",
    "    if df_orders.empty :\n",
    "        logger.warning(\"No valid orders after customer_id foreign key check for Orders table. Returning empty DataFrames.\")\n",
    "        return pd.DataFrame(columns=df_all_order_items.columns), pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Derived Orders table. Shape: {df_orders.shape}\")\n",
    "    if not df_orders.empty:\n",
    "         logger.info(f\"Sample of derived Orders with amount_paid_total:\\n{df_orders[['order_id', 'amount_paid_total', 'order_status']].head()}\")\n",
    "    \n",
    "    final_order_item_db_cols = [\n",
    "        'order_item_id', 'order_id', 'product_id', 'customer_id', \n",
    "        'quantity', 'unit_price', 'line_item_total_value', 'line_item_discount', \n",
    "        'line_item_tax', 'line_item_shipping_fee', 'source_file', \n",
    "        'original_line_identifier', 'last_updated_pipeline'\n",
    "    ]\n",
    "    \n",
    "    for col in final_order_item_db_cols:\n",
    "        if col not in df_all_order_items.columns:\n",
    "            df_all_order_items[col] = None \n",
    "            logger.warning(f\"Column '{col}' for final OrderItems was missing and added as None.\")\n",
    "            \n",
    "    df_order_items_for_db = df_all_order_items[final_order_item_db_cols].copy()\n",
    "    \n",
    "    df_order_items_for_db = df_order_items_for_db[df_order_items_for_db['order_id'].isin(df_orders['order_id'])]\n",
    "    logger.info(f\"Final OrderItems for DB after filtering by valid orders. Shape: {df_order_items_for_db.shape}\")\n",
    "\n",
    "    if not df_orders.empty:\n",
    "        try:\n",
    "            with db_engine.connect() as connection:\n",
    "                connection.execute(text(\"DELETE FROM OrderItems;\"))\n",
    "                connection.execute(text(\"DELETE FROM Orders;\"))\n",
    "                connection.commit()\n",
    "                logger.info(\"Cleared existing Orders and OrderItems data from DB for fresh load.\")\n",
    "            df_orders.to_sql('Orders', db_engine, if_exists='append', index=False)\n",
    "            logger.info(f\"{len(df_orders)} records loaded/appended into Orders table.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Orders data to SQLite: {e}. OrderItems will not be loaded.\")\n",
    "            return pd.DataFrame(), pd.DataFrame() \n",
    "    else:\n",
    "        logger.warning(\"Derived Orders DataFrame is empty. Nothing to load to Orders table.\")\n",
    "        return df_order_items_for_db, df_orders \n",
    "\n",
    "    if not df_order_items_for_db.empty:\n",
    "        try:\n",
    "            df_order_items_for_db.to_sql('OrderItems', db_engine, if_exists='append', index=False)\n",
    "            logger.info(f\"{len(df_order_items_for_db)} records loaded/appended into OrderItems table.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading OrderItems data to SQLite: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Final OrderItems DataFrame is empty. Nothing to load to OrderItems table.\")\n",
    "        \n",
    "    return df_order_items_for_db, df_orders\n",
    "\n",
    "logger.info(\"Preparing to call etl_combine_orders_and_load...\")\n",
    "\n",
    "items1_for_combine = df_order_items_source1 if 'df_order_items_source1' in globals() and isinstance(df_order_items_source1, pd.DataFrame) else pd.DataFrame()\n",
    "items2_for_combine = df_order_items_source2 if 'df_order_items_source2' in globals() and isinstance(df_order_items_source2, pd.DataFrame) else pd.DataFrame()\n",
    "\n",
    "if items1_for_combine.empty:\n",
    "    logger.warning(\"df_order_items_source1 is empty or not a DataFrame going into combine step.\")\n",
    "else:\n",
    "    logger.info(f\"df_order_items_source1 to be combined has shape: {items1_for_combine.shape} and columns: {items1_for_combine.columns.tolist()}\")\n",
    "\n",
    "if items2_for_combine.empty:\n",
    "    logger.warning(\"df_order_items_source2 is empty or not a DataFrame going into combine step.\")\n",
    "else:\n",
    "    logger.info(f\"df_order_items_source2 to be combined has shape: {items2_for_combine.shape} and columns: {items2_for_combine.columns.tolist()}\")\n",
    "\n",
    "\n",
    "if 'existing_customer_ids_set' not in globals() or 'existing_product_ids_set' not in globals():\n",
    "    logger.error(\"CRITICAL: existing_customer_ids_set or existing_product_ids_set not found. Re-run previous ETL steps for Customers and Products.\")\n",
    "    existing_customer_ids_set = set() \n",
    "    existing_product_ids_set = set()\n",
    "\n",
    "df_final_order_items_loaded, df_final_orders_loaded = etl_combine_orders_and_load(\n",
    "    items1_for_combine,\n",
    "    items2_for_combine,\n",
    "    engine,\n",
    "    existing_customer_ids_set, \n",
    "    existing_product_ids_set\n",
    ")\n",
    "\n",
    "if df_final_orders_loaded is not None and not df_final_orders_loaded.empty:\n",
    "    logger.info(f\"Orders table loaded successfully. Final shape: {df_final_orders_loaded.shape}\")\n",
    "    print(\"\\n--- Final Orders Data (Sample from DB load) ---\")\n",
    "    print(df_final_orders_loaded.head())\n",
    "else:\n",
    "    logger.warning(\"Final Orders DataFrame is empty or None after ETL combine step. No orders loaded.\")\n",
    "\n",
    "if df_final_order_items_loaded is not None and not df_final_order_items_loaded.empty:\n",
    "    logger.info(f\"OrderItems table loaded successfully. Final shape: {df_final_order_items_loaded.shape}\")\n",
    "    print(\"\\n--- Final OrderItems Data (Sample from DB load) ---\")\n",
    "    print(df_final_order_items_loaded.head())\n",
    "else:\n",
    "    logger.warning(\"Final OrderItems DataFrame is empty or None after ETL combine step. No order items loaded.\")\n",
    "\n",
    "logger.info(\"--- ETL Pipeline (Combine Orders & OrderItems) Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c171eb-ef1e-48ab-9aa2-4fbf9edca2a7",
   "metadata": {},
   "source": [
    "# Cell 2.5.4: AI-Assisted Schema Reconciliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eafa9869-24ef-4ca0-b585-54b7c9c9c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinmay/NexusFlow/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-23 08:48:44,656 - INFO - GEMINI_API_KEY environment variable not found.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Gemini API Key (or press Enter to skip AI features):  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:48:46,674 - INFO - Gemini 1.5 Pro model configured successfully.\n",
      "2025-06-23 08:48:46,677 - INFO - Attempting AI-assisted schema mapping...\n",
      "2025-06-23 08:48:46,679 - INFO - Sending schema mapping prompt to Gemini (first 500 chars):\n",
      "You are a data engineering assistant specializing in schema mapping and reconciliation.\n",
      "Given column names from two different source dataframes and a target canonical database schema, \n",
      "your task is to suggest the most likely mappings from each source's columns to the target schema's columns.\n",
      "Consider common naming conventions, abbreviations, and semantic similarities (e.g., 'cust_id' and 'customer_identifier' might both map to 'customer_id').\n",
      "\n",
      "--- Source A Details ---\n",
      "Source A Name: Customers Me...\n",
      "2025-06-23 08:48:47,364 - ERROR - Error calling Gemini API for schema mapping: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "]\n",
      "2025-06-23 08:48:47,366 - WARNING - AI schema mapping did not return suggestions (this may be due to an API error like rate limiting).\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5.4: AI-Assisted Schema Reconciliation \n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import getpass # For securely getting API key in a notebook environment\n",
    "\n",
    "# --- AI Reconciliation Configuration ---\n",
    "# Global variable to store the Gemini model instance once initialized\n",
    "gemini_model = None\n",
    "gemini_api_key_provided = False\n",
    "\n",
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configures the Gemini API with a user-provided key.\n",
    "    Returns the initialized model or None if configuration fails.\n",
    "    \"\"\"\n",
    "    global gemini_model, gemini_api_key_provided\n",
    "    \n",
    "    if gemini_model is not None: # Already configured\n",
    "        logger.info(\"Gemini model already configured.\")\n",
    "        return gemini_model\n",
    "\n",
    "    try:\n",
    "        # Attempt to get API key from environment variable first (more secure for repeated runs)\n",
    "        api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not api_key:\n",
    "            logger.info(\"GEMINI_API_KEY environment variable not found.\")\n",
    "            api_key_input = getpass.getpass(\"Enter your Gemini API Key (or press Enter to skip AI features): \")\n",
    "            if not api_key_input:\n",
    "                logger.warning(\"No Gemini API Key provided. AI-assisted features will be skipped.\")\n",
    "                gemini_api_key_provided = False\n",
    "                return None\n",
    "            api_key = api_key_input\n",
    "        \n",
    "        genai.configure(api_key=api_key)\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro-latest') # Using the specified model\n",
    "        gemini_model = model\n",
    "        gemini_api_key_provided = True\n",
    "        logger.info(\"Gemini 1.5 Pro model configured successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring Gemini: {e}. AI-assisted features may not work.\")\n",
    "        gemini_api_key_provided = False\n",
    "        return None\n",
    "\n",
    "def get_ai_schema_mapping_suggestions(source_name_a, columns_a, source_name_b, columns_b, target_schema_dict):\n",
    "    \"\"\"\n",
    "    Uses Gemini to suggest schema mappings.\n",
    "    target_schema_dict should be like: {'table_name': ['col1', 'col2', ...]}\n",
    "    \"\"\"\n",
    "    global gemini_model\n",
    "    if not gemini_model:\n",
    "        logger.warning(\"Gemini model not configured. Skipping AI schema mapping.\")\n",
    "        return None\n",
    "\n",
    "    prompt_parts = [\n",
    "        \"You are a data engineering assistant specializing in schema mapping and reconciliation.\",\n",
    "        \"Given column names from two different source dataframes and a target canonical database schema, \",\n",
    "        \"your task is to suggest the most likely mappings from each source's columns to the target schema's columns.\",\n",
    "        \"Consider common naming conventions, abbreviations, and semantic similarities (e.g., 'cust_id' and 'customer_identifier' might both map to 'customer_id').\",\n",
    "        \"\\n--- Source A Details ---\",\n",
    "        f\"Source A Name: {source_name_a}\",\n",
    "        f\"Source A Columns: {', '.join(columns_a)}\",\n",
    "        \"\\n--- Source B Details ---\",\n",
    "        f\"Source B Name: {source_name_b}\",\n",
    "        f\"Source B Columns: {', '.join(columns_b)}\",\n",
    "        \"\\n--- Target Canonical Schema ---\"\n",
    "    ]\n",
    "    for table, cols in target_schema_dict.items():\n",
    "        prompt_parts.append(f\"Target Table '{table}': {', '.join(cols)}\")\n",
    "\n",
    "    prompt_parts.append(\"\\n--- Instructions ---\")\n",
    "    prompt_parts.append(f\"1. For '{source_name_a}', provide mappings to any relevant target table and column.\")\n",
    "    prompt_parts.append(f\"2. For '{source_name_b}', provide mappings to any relevant target table and column.\")\n",
    "    prompt_parts.append(\"3. If a source column does not seem to map to any target column, indicate with 'NO_CLEAR_TARGET'.\")\n",
    "    prompt_parts.append(\"4. If a source column could map to multiple targets, list the most probable or note the ambiguity.\")\n",
    "    prompt_parts.append(\"Provide the output strictly as a single JSON object. Do not include any text or markdown formatting before or after the JSON block.\")\n",
    "    prompt_parts.append(\"\\nExample JSON output format:\")\n",
    "    prompt_parts.append(\"\"\"\n",
    "    {\n",
    "      \"source_a_mappings\": {\n",
    "        \"client_ref\": \"Customers.customer_id\",\n",
    "        \"purchase_dt\": \"Orders.order_date\",\n",
    "        \"item_sku\": \"Products.product_id\",\n",
    "        \"weird_legacy_col\": \"NO_CLEAR_TARGET\"\n",
    "      },\n",
    "      \"source_b_mappings\": {\n",
    "        \"CustomerID\": \"Customers.customer_id\",\n",
    "        \"orderTimestamp\": \"Orders.order_date\",\n",
    "        \"productCode\": \"Products.product_id\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    prompt = \"\\n\".join(prompt_parts)\n",
    "    logger.info(f\"Sending schema mapping prompt to Gemini (first 500 chars):\\n{prompt[:500]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        # Attempt to parse the response as JSON\n",
    "        try:\n",
    "            # Gemini might wrap JSON in ```json ... ```, so we extract it\n",
    "            match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", response.text)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "            else:\n",
    "                json_str = response.text # Assume it's raw JSON\n",
    "            \n",
    "            suggestions = json.loads(json_str)\n",
    "            logger.info(\"Successfully received and parsed schema mapping suggestions from Gemini.\")\n",
    "            return suggestions\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(f\"Gemini response was not valid JSON. Raw response:\\n{response.text}\")\n",
    "            return {\"error\": \"Invalid JSON response\", \"raw_text\": response.text}\n",
    "        except Exception as e_parse:\n",
    "            logger.error(f\"Error processing Gemini response: {e_parse}. Raw response:\\n{response.text}\")\n",
    "            return {\"error\": str(e_parse), \"raw_text\": response.text}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling Gemini API for schema mapping: {e}\")\n",
    "        # Note: The original '429 Rate Limit Exceeded' error would be caught here.\n",
    "        # This is an API usage issue, not a code bug. It means the free tier limit was reached.\n",
    "        return None\n",
    "\n",
    "# --- Define Target Schema for AI Mapping (simplified for demonstration) ---\n",
    "target_schema_for_ai = {\n",
    "    \"Customers\": ['customer_id', 'customer_name', 'email', 'phone', 'address_street', 'address_city', 'address_state', 'address_postal_code', 'registration_date', 'status', 'segment'],\n",
    "    \"Products\": ['product_id', 'product_name', 'description', 'category', 'brand', 'price', 'cost', 'stock_quantity'],\n",
    "    \"Orders\": ['order_id', 'customer_id', 'order_date', 'order_status', 'payment_method', 'shipping_cost_total', 'tax_total', 'discount_total', 'order_total_value_net'],\n",
    "    \"OrderItems\": ['order_item_id', 'order_id', 'product_id', 'quantity', 'unit_price', 'line_item_total_value']\n",
    "}\n",
    "\n",
    "# --- Attempt to Configure Gemini ---\n",
    "# This will prompt for API key if not set in environment\n",
    "# NOTE: This relies on the first cell in the notebook (which sets up logging) having been run.\n",
    "configure_gemini() \n",
    "\n",
    "# --- Execute AI Schema Mapping Suggestion (if API key was provided) ---\n",
    "if gemini_api_key_provided and gemini_model:\n",
    "    logger.info(\"Attempting AI-assisted schema mapping...\")\n",
    "    \n",
    "    cust_raw_cols = df_customers_raw.columns.tolist() if 'df_customers_raw' in globals() and not df_customers_raw.empty else []\n",
    "    prod_raw_cols = df_products_raw.columns.tolist() if 'df_products_raw' in globals() and not df_products_raw.empty else []\n",
    "    \n",
    "    if cust_raw_cols and prod_raw_cols:\n",
    "        ai_schema_suggestions = get_ai_schema_mapping_suggestions(\n",
    "            source_name_a=\"Customers Messy JSON\",\n",
    "            columns_a=cust_raw_cols,\n",
    "            source_name_b=\"Products Inconsistent JSON\",\n",
    "            columns_b=prod_raw_cols,\n",
    "            target_schema_dict=target_schema_for_ai\n",
    "        )\n",
    "\n",
    "        if ai_schema_suggestions:\n",
    "            print(\"\\n--- AI Schema Mapping Suggestions ---\")\n",
    "            print(json.dumps(ai_schema_suggestions, indent=2))\n",
    "        else:\n",
    "            logger.warning(\"AI schema mapping did not return suggestions (this may be due to an API error like rate limiting).\")\n",
    "    else:\n",
    "        logger.warning(\"Raw customer or product dataframes are not available for AI schema mapping.\")\n",
    "else:\n",
    "    logger.info(\"Gemini AI features skipped as API key was not provided or model initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6874dd-11c2-4efe-b4eb-b7873c319c0f",
   "metadata": {},
   "source": [
    "# Phase 5: Conclusion & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d09f39-889b-4588-aa36-3abb6cb76bf4",
   "metadata": {},
   "source": [
    "It started with Phase 1: Data Discovery & Analysis, where I explored four messy datasets to find out data quality issues, understand the meaning of each field, and identify relationships between them. These insights helped with the development of the ETL pipelines in Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bc84f-a489-4882-a22c-a250949e4bbc",
   "metadata": {},
   "source": [
    "I built robust ETL pipelines for the Customers, Products, and OrderItems datasets, which came from two different sources (reconciliation_challenge_data.csv and orders_unstructured_data.csv). These pipelines included custom utility functions for cleaning, standardizing (e.g., names, dates, numbers, IDs), and transforming the data. Cleaned data was then stored in a normalized SQLite database (unified_ecommerce.db), with a well-defined schema that included primary keys, foreign keys, and indexes. I also created unified Orders and OrderItems tables from the combined sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5f7a6-3526-4564-92c6-a320ba7ad039",
   "metadata": {},
   "source": [
    "In Phase 4: Advanced Challenge, I proposed and implemented a basic framework to integrate Google Geminis API. This showed how AI could be used to suggest schema mappings to reconcile mismatched fields across sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df808312-740d-4a07-898c-3ab88d476fb9",
   "metadata": {},
   "source": [
    "Key Challenges Faced and Solutions\n",
    "- Multiple Inconsistent ID Fields\n",
    "- Unstructured and Messy Text Fields\n",
    "- Incorrect Financial Totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bba5d1-9cfe-422b-aa47-731d193a44ff",
   "metadata": {},
   "source": [
    "This was a great opportunity to learn and apply core data engineering skills and get to work on a real world problem. The AI reconciliation part was very interesting.\n",
    "Thankyou for the opportunity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
